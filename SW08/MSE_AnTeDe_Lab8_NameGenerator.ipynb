{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://www.msengineering.ch/typo3conf/ext/msengineering/Resources/Public/Images/Logo/mse-full.svg \"MSE Logo\") \n",
    "\n",
    "# AnTeDe Practical Work 8: Name Generation with RNN\n",
    "\n",
    "by Fabian MÃ¤rki\n",
    "\n",
    "## Summary\n",
    "The aim of this lab is to get an understanding of building a RNN model using Keras. The task is to train a character-level language models that generates new baby names (but feel free to change this to e.g. new start-up names or city names etc.). \n",
    "\n",
    "### Source\n",
    "- https://github.com/JKH4/name-generator/blob/master/dev/2018-05-18_JKH_NameGen-Main.ipynb\n",
    "\n",
    "This lab contains assigments (although most of the code is given). <font color='red'>Questions are written in red.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, GRU, SimpleRNN, Bidirectional, InputLayer\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from more_itertools import sort_together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    print(\"Run on GPU\")\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    #config.gpu_options.per_process_gpu_memory_fraction = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Task\n",
    "\n",
    "Below you find working code that needs some improvement.\n",
    "\n",
    "<font color='red'>Your task is to get hands on experience with Keras and RNNs by trying different options on how to build a RNN and on how to tune it. \n",
    "<br>After you created a model with good performance, please write a short summary about your experience: what works well, what did not work well, what influenced the performance of the model, did you experience strange behaviors, how did you analyze the data, how do you estimate the performance of your model, what possible further improvements you can consider.</font>\n",
    "\n",
    "<font color='red'>Modify the function `create_model` according your intuition on how the model could be improved. Options you might want to try (please experiment with at least four of them):</font>\n",
    "\n",
    "- try different RNN types (SimpleRNN, LSTM, GRU - see [here](https://keras.io/layers/recurrent))\n",
    "- try different number of RNN units (see `parameters`)\n",
    "- use regularization techniques (e.g. dropout)\n",
    "- use options provided by the RNN types (e.g. arguments \"dropout\" and \"recurrent_dropout\" - see [here](https://keras.io/layers/recurrent))\n",
    "- stack several RNN layers (see [here](https://keras.io/getting-started/sequential-model-guide/) and search for \"Stacked LSTM for sequence classification\")\n",
    "- try different optimizers (see [here](https://keras.io/optimizers/))\n",
    "- get more inspiration from [here](https://ruder.io/deep-learning-nlp-best-practices)\n",
    "\n",
    "You might also want to have a look at the `parameters` variable below and modify it according your needs.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>Please provide a summary of your experience in this box.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(parameters):\n",
    "    length_of_sequence = parameters[\"trainset_infos\"]['length_of_sequence']\n",
    "    number_of_chars = parameters[\"trainset_infos\"]['number_of_chars']\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(length_of_sequence, number_of_chars)))\n",
    "    model.add(SimpleRNN(parameters[\"rnn_units\"]))\n",
    "    model.add(Dense(number_of_chars, activation='softmax'))\n",
    "    \n",
    "    if parameters.get(\"verbose\"):\n",
    "        model.summary()\n",
    "        \n",
    "    return model\n",
    "\n",
    "def load_data(parameters):\n",
    "    length_of_sequence = 5\n",
    "    padding_start = '#'\n",
    "    padding_end = '*'\n",
    "    file_url = parameters[\"file_url\"]\n",
    "\n",
    "    text = ''\n",
    "    with io.open(get_file(os.path.basename(file_url), origin=file_url), encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "\n",
    "    names = pd.read_csv(io.StringIO(text), names=['name'], comment='#', header=None)\n",
    "    names['name'] = names['name'].map(lambda n: n.replace(padding_start, ''))    # replace characters used for training\n",
    "    names['name'] = names['name'].map(lambda n: n.replace(padding_end, ''))    # replace characters used for training\n",
    "    names['name'] = names['name'].map(lambda n: padding_start + n + padding_end) \n",
    "    \n",
    "    data_dict = {}\n",
    "    data_dict['name_list'] = names['name']\n",
    "    data_dict['char_list'] = sorted(list(set(data_dict['name_list'].str.cat() + '*')))\n",
    "    data_dict['char_to_ix'] = { ch:i for i,ch in enumerate(data_dict['char_list']) }\n",
    "    data_dict['ix_to_char'] = { i:ch for i,ch in enumerate(data_dict['char_list']) }\n",
    "           \n",
    "    # Extract target names to list (currently '#name*')\n",
    "    training_names = data_dict['name_list'].tolist()\n",
    "    \n",
    "    # Extract padding characters\n",
    "    padding_start = training_names[0][0]\n",
    "    padding_end = training_names[0][-1]\n",
    "\n",
    "    # Extract target character convertors\n",
    "    # This will be used to convert a character to its \"one hot index\" and vice versa (cf Keras to_categorical())\n",
    "    c2i = data_dict['char_to_ix']\n",
    "    i2c = data_dict['ix_to_char']\n",
    "    \n",
    "    # Extract the target number of characters in all target names\n",
    "    # This will be used to convert character index in its \"one hot\" representation (cf Keras to_categorical())\n",
    "    number_of_chars = len(data_dict['char_list'])\n",
    "    \n",
    "    # Pad target names with enough (lengh_of_sequence) padding characters (result '##...##name**...**' )\n",
    "    # The goal is  be sure that, for each name, the first training data is X[0] = '##...##'\n",
    "    # and Y[0] = First actual character of the name\n",
    "    training_names = [\n",
    "        padding_start * (length_of_sequence - 1) + n + padding_end * (length_of_sequence - 1) for n in training_names\n",
    "    ]\n",
    "\n",
    "    # Init X and Y as list\n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "\n",
    "    # Init counter for visual feedback\n",
    "    counter = 0 if parameters[\"verbose\"] else None\n",
    "    \n",
    "    for name in training_names:\n",
    "        # Slide a window on the name, one character at a time\n",
    "        for i in range(max(1, len(name) - length_of_sequence)):\n",
    "            # Extract the new sequence and the character following this sequence\n",
    "            new_sequence = name[i:i + length_of_sequence]\n",
    "            target_char = name[i + length_of_sequence]\n",
    "            \n",
    "            # Add the new sequence to X (input of the model)\n",
    "            X_list.append([to_categorical(c2i[c], number_of_chars) for c in new_sequence])\n",
    "            # Add the following character to Y (target to be predicted by the model)\n",
    "            Y_list.append(to_categorical(c2i[target_char], number_of_chars))\n",
    "\n",
    "        # visual feedback\n",
    "        if parameters[\"verbose\"]:\n",
    "            counter += 1\n",
    "            print(counter) if counter % 100 == 0 else print('.', end='')\n",
    "            \n",
    "    # make sure number of elements allignes with batch size\n",
    "    offset = len(X_list) % parameters[\"batch_size\"]\n",
    "    if offset != 0:\n",
    "        elements_to_copy = parameters[\"batch_size\"] - offset\n",
    "        X_list.extend(X_list[:elements_to_copy])\n",
    "        Y_list.extend(Y_list[:elements_to_copy])\n",
    "        \n",
    "    # Convert X and Y to numpy array\n",
    "    x_train = np.array(X_list)\n",
    "    y_train = np.array(Y_list)\n",
    "    \n",
    "    # Extract the number of training samples\n",
    "    m = len(x_train)\n",
    "    \n",
    "    # Create a description of the trainset\n",
    "    parameters[\"trainset_infos\"] = {\n",
    "        'length_of_sequence': length_of_sequence,\n",
    "        'number_of_chars': number_of_chars,\n",
    "        'm': m,\n",
    "        'padding_start': padding_start,\n",
    "        'padding_end': padding_end,\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        '\\n{} names split in {} training sequence of {} encoded chars !'.format(counter, m, length_of_sequence)\n",
    "    ) if parameters[\"verbose\"] else None\n",
    "\n",
    "    # Visual feedbacks\n",
    "    if parameters[\"verbose\"]:\n",
    "        print('X shape: {}'.format(x_train.shape))\n",
    "        print('Y shape: {}'.format(y_train.shape))\n",
    "\n",
    "        print('X[0] = {}'.format(x_train[0]))\n",
    "        print('Y[0] = {}'.format(y_train[0]))\n",
    "\n",
    "        print('Training set size: {}'.format(m))\n",
    "        print('length_of_sequence: {}'.format(length_of_sequence))\n",
    "        print('number_of_chars: {}'.format(number_of_chars))\n",
    "        print('some names: {}'.format(names['name'][:5]))\n",
    "    \n",
    "                 \n",
    "    parameters[\"x_train\"] = x_train\n",
    "    parameters[\"y_train\"] = y_train\n",
    "    parameters[\"word2index\"] = c2i\n",
    "    parameters[\"index2word\"] = i2c                \n",
    "\n",
    "\n",
    "def load_embeddings(word2index, word2embedding, embedding_dim, input_length = None, trainable=False):\n",
    "    #return gensim_embedding_model.get_keras_embedding(train_embeddings=train_embeddings)\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(word2index) + 1, embedding_dim))\n",
    "    \n",
    "    for word, i in word2index.items():\n",
    "        embedding_vector = word2embedding.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    if input_length is None:\n",
    "        return Embedding(len(word2index) + 1, \n",
    "                         embedding_dim,\n",
    "                         weights=[embedding_matrix],\n",
    "                         trainable=trainable)\n",
    "    else:\n",
    "        return Embedding(len(word2index) + 1, \n",
    "                         embedding_dim,\n",
    "                         weights=[embedding_matrix],\n",
    "                         input_length=input_length,\n",
    "                         trainable=trainable)\n",
    "\n",
    "def create_embedding(parameters):\n",
    "    if parameters[\"embedding_use_pretrained\"]:\n",
    "        embedding_model = parameters[\"embedding_model\"]\n",
    "        word2embedding = {word: embedding_model[word] for word, vector in embedding_model.vocab.items()}\n",
    "        embedding_dim = embedding_model.vector_size\n",
    "        embedding_layer = load_embeddings(parameters[\"word2index\"], word2embedding, embedding_dim)\n",
    "        return embedding_layer\n",
    "    else:\n",
    "        return Embedding(parameters[\"max_words\"], parameters[\"embedding_dim\"], input_length=parameters[\"maxlen\"])\n",
    "\n",
    "\n",
    "def compile_model(model, parameters):\n",
    "    optimizer = Adam(lr = parameters[\"learning_rates\"][parameters[\"iter\"]])\n",
    "\n",
    "    model.compile(loss=parameters[\"loss_function\"], optimizer=optimizer, metrics = parameters[\"metrics\"])\n",
    "\n",
    "        \n",
    "def train_model(model, parameters):\n",
    "    i = parameters[\"iter\"]\n",
    "    \n",
    "    # Train the model\n",
    "    h = model.fit(\n",
    "        parameters[\"x_train\"], parameters[\"y_train\"],\n",
    "        validation_data = parameters.get(\"validation_data\"),\n",
    "        batch_size = parameters[\"batch_size\"],\n",
    "        callbacks = parameters.get(\"callbacks\"),\n",
    "        initial_epoch = parameters[\"total_epochs\"],\n",
    "        epochs = parameters[\"total_epochs\"] + parameters[\"epochs_to_run\"][i]\n",
    "    )\n",
    "\n",
    "    history = parameters[\"history\"]\n",
    "    # Update history\n",
    "    for key, val in h.history.items():\n",
    "        col = history.get(key)\n",
    "        \n",
    "        if col is None:\n",
    "            col = np.array([])\n",
    "        \n",
    "        history[key] = np.append(col, val)\n",
    "        \n",
    "    \n",
    "    # Update the training session info\n",
    "    parameters['total_epochs'] += parameters['epochs_to_run'][i]\n",
    "    \n",
    "    \n",
    "def plot_class_balance(y, title=''):\n",
    "    (unique, counts) = np.unique(y, return_counts=True)\n",
    "    (unique, counts) = sort_together([unique, counts])\n",
    "\n",
    "    plt.bar(unique, counts, align='center')\n",
    "    plt.xticks(np.arange(len(unique)), unique)\n",
    "    plt.xlabel('label')\n",
    "    plt.ylabel('count')\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_confusion_matrix(y_true, y_pred, title=''):\n",
    "    classes = list(set(list(y_true) + list(y_pred)))\n",
    "    classes.sort()\n",
    "\n",
    "    cmm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print('Set Population: {}'.format(cmm.sum()))\n",
    "    print('Accuracy: {:.4f}'.format(float(cmm.trace()) / cmm.sum()))\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cmm / cmm.sum(), interpolation='nearest', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.ylim(-0.5, len(classes)-0.5)\n",
    "\n",
    "    if classes is not None:\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45, size='x-large')\n",
    "        plt.yticks(tick_marks, classes, size='x-large')\n",
    "\n",
    "    for y in range(cmm.shape[0]):\n",
    "        for x in range(cmm.shape[1]):\n",
    "            if cmm[y, x] > 0:\n",
    "                plt.text(x, y, '%.0i' % cmm[y, x],\n",
    "                         horizontalalignment='center',\n",
    "                         verticalalignment='center')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_training_session(parameters, plots=[\"accuracy\", \"val_accuracy\"]):\n",
    "    history = parameters[\"history\"]\n",
    "    \n",
    "    x = range(len(history[plots[0]]))\n",
    "    \n",
    "    for label in plots:\n",
    "        vals = history.get(label)\n",
    "        if vals is not None:\n",
    "            plt.plot(x, vals, label = label)\n",
    "        else:\n",
    "            print(\"There is no data for\", label)\n",
    "    \n",
    "    plt.xlabel('epoch')\n",
    "    plt.title('Progress')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def generate_name(model, parameters, start_char = None, name_max_length = 25):\n",
    "    '''\n",
    "    Generate some name with the RNN model\n",
    "    \n",
    "    ## Inputs:\n",
    "    model (Keras model): \n",
    "    parameters (dict): the parameters\n",
    "    name_max_length (integer): max size of the generated name\n",
    "    verbose (boolean): show some feedbacks\n",
    "    ## Outputs:\n",
    "    generated_name (string): name generated by the RNN\n",
    "    (probability, gap): few numbers about this generated name\n",
    "        probability: probability to generate this name (cummulative probability to select each character)\n",
    "        ecart: gap between best name and this name (cummulative sum of gaps between selected character and best character)\n",
    "        \n",
    "    '''\n",
    "    trainset_infos = parameters[\"trainset_infos\"]\n",
    "    # Extract the number of unique character in trainset\n",
    "    dict_size = trainset_infos[\"number_of_chars\"]\n",
    "    \n",
    "    # Extract the size of an input sequence\n",
    "    sequence_length = trainset_infos[\"length_of_sequence\"]\n",
    "    \n",
    "    # Extract utils dictionnary to convert character to one hot index and vice versa\n",
    "    # in this context 'word' is meant to be a character\n",
    "    i2c = parameters[\"index2word\"]\n",
    "    c2i = parameters[\"word2index\"]\n",
    "    \n",
    "    # Extract padding character\n",
    "    padding_start = trainset_infos[\"padding_start\"]\n",
    "    \n",
    "    # Init a name full of padding_start character\n",
    "    generated_name = padding_start * (sequence_length + name_max_length)\n",
    "\n",
    "    # Init counters\n",
    "    probability = 1\n",
    "    gap = 0\n",
    "\n",
    "    if start_char is not None:\n",
    "        generated_name = generated_name[:(sequence_length - 1)] + start_char + generated_name[sequence_length:]\n",
    "    \n",
    "    # Generate new character from current sequence\n",
    "    for i in range(name_max_length):\n",
    "        # Extract current sequence from generated character\n",
    "        x_char = generated_name[i:i+sequence_length]\n",
    "        \n",
    "        # Convert current sequence to one hot vector\n",
    "        x_cat = np.array([[to_categorical(c2i[c], dict_size) for c in x_char]])\n",
    "        \n",
    "        # Predict new character probabilities\n",
    "        # Actually this output a list of probabilities for each character\n",
    "        p = model.predict(x_cat)\n",
    "\n",
    "        # Extract the best character (and its probability)\n",
    "        best_char = i2c[np.argmax(p)]\n",
    "        best_char_prob = np.max(p)\n",
    "\n",
    "        # Choose a random character index according to their probabilities (and its probability)\n",
    "        new_char_index = np.random.choice(range(dict_size), p = p.ravel())\n",
    "        new_char_prob = p[0][new_char_index]\n",
    "        \n",
    "        # Convert the index to an actual character\n",
    "        new_char = i2c[new_char_index]\n",
    "                \n",
    "        # Update the generated name with the new character\n",
    "        generated_name = generated_name[:sequence_length+i] + new_char + generated_name[sequence_length+i+1:]\n",
    "        \n",
    "        # Update counters\n",
    "        probability *= new_char_prob # probabilities are multiplied\n",
    "        gap += best_char_prob-new_char_prob # gaps are summed\n",
    "\n",
    "        # Show some feedbacks\n",
    "        if parameters[\"verbose\"]:\n",
    "            print(\n",
    "                'i={} new_char: {} ({:.3f}) [best:  {} ({:.3f}), diff: {:.3f}, prob: {:.3f}, gap: {:.3f}]'.format(\n",
    "                    i,\n",
    "                    new_char,\n",
    "                    new_char_prob,\n",
    "                    best_char,\n",
    "                    best_char_prob,\n",
    "                    best_char_prob-new_char_prob,\n",
    "                    probability,\n",
    "                    gap\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Stop the prediction loop if it reached a 'padding_end' character\n",
    "        if (new_char == trainset_infos['padding_end']):\n",
    "            break\n",
    "    \n",
    "    # Clean the generated name\n",
    "    generated_name = generated_name.strip('#*')\n",
    "    \n",
    "    # Show some feedbacks\n",
    "    print('{} (probs: {:.6f}, gap: {:.6f})'.format(generated_name, probability, gap)) if parameters[\"verbose\"] else None\n",
    "\n",
    "    return generated_name, {'probability': probability, 'gap': gap}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>You will need to tune the parameters (you probably want to have a look at 'rnn_units', and 'epochs_to_run').  Please indicate as comments the values you tried, and the best values you keep.</font>\n",
    "\n",
    "<font color='red'>The number of elements in the array 'epochs_to_run' and 'learning_rates' defines the number of epochs and learning rate per epoch the model should be trained.  E.g., in the first training round use 3 epochs with a learning rate of 0.03, in a second training round use 5 epochs and decrease the learning rate to 0.001, etc.) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"verbose\": True,\n",
    "    \"max_words\": 20000,\n",
    "    \"max_sequence_length\": 1000,\n",
    "    \"maxlen\": 100,\n",
    "    \"rnn_units\": 8,\n",
    "    \"embedding_use_pretrained\": False,\n",
    "    \"embedding_fine_tune\": False,\n",
    "    \"embedding_dim\": 128,\n",
    "    \"iter\": 0,\n",
    "    \"epochs_to_run\": [3, 5, 7],\n",
    "    \"learning_rates\": [0.03, 0.001, 0.0003],\n",
    "    \"total_epochs\": 0,\n",
    "    \"loss_function\": \"categorical_crossentropy\", \n",
    "    \"metrics\": [\"accuracy\"],\n",
    "    \"batch_size\": 32,\n",
    "    \"history\": {},\n",
    "    \"file_url\": \"https://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/male.txt\"\n",
    "    #\"file_url\": \"https://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/female.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters[\"iter\"] = 0\n",
    "parameters[\"total_epochs\"] = 0\n",
    "parameters[\"history\"] = {}\n",
    "\n",
    "model = create_model(parameters)\n",
    "compile_model(\n",
    "    model,\n",
    "    parameters\n",
    ")\n",
    "\n",
    "for i in range(len(parameters[\"epochs_to_run\"])):\n",
    "    train_model(model, parameters)\n",
    "    parameters[\"iter\"] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_session(parameters, plots=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_name(model, parameters, start_char = 'j')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_name(model, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
