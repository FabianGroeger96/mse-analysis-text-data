{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HVF9ICdHfOS"
   },
   "source": [
    "# AnTeDe Lab 13: Named Entity Recognition with a bidirectional LSTM and Conditional Random Fields\n",
    "\n",
    "## Session goal\n",
    "In this session we'll implement a Named Entity Recognition system using a bidirectional LSTM and Conditional Random Fields based on:\n",
    "*   Z. Huang et al., **Bidirectional LSTM-CRF Models for Sequence Tagging**, Arxiv preprint, 2015\n",
    "*   G. Lample et al., **Neural Architectures for Named Entity Recognition**, NAACL 2016\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ImuPxi2wHfOW",
    "outputId": "3061a5aa-a16b-48ee-f2d2-665a786a9735"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Install extra-dependencies\n",
    "#! pip -q install git+https://www.github.com/keras-team/keras-contrib.git sklearn-crfsuite\n",
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2abVlBDHfOa"
   },
   "source": [
    "Here we set the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nm0EwDS-HfOa"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512  \n",
    "EPOCHS = 100 \n",
    "MAX_LEN = 80  \n",
    "EMBEDDING = 300 \n",
    "LSTM_UNITS = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Lq6Iay7HfOc"
   },
   "source": [
    "Here we process the CONLL 2003 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0auL6zhaapug",
    "outputId": "78e42be6-dd55-43c0-8e6a-42d9e0042500"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=f7b122c083a00dcd56e4ccb8d74f1e41707aa60e7d8c9e97ddf3902729f8f468\n",
      "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wget\n",
    "import wget\n",
    "\n",
    "validation_url='https://www.dropbox.com/s/ed6l16yvsi2ngjl/eng.testa?dl=1'\n",
    "test_url='https://www.dropbox.com/s/i14erxmgcx2bdxc/eng.testb?dl=1'\n",
    "training_url='https://www.dropbox.com/s/0lmclaa237w20le/eng.train?dl=1'\n",
    "\n",
    "import os\n",
    "if 'CONLL-2003' not in os.listdir():\n",
    "    os.mkdir('CONLL-2003')\n",
    "\n",
    "    wget.download(validation_url, 'CONLL-2003') \n",
    "    wget.download(test_url, 'CONLL-2003') \n",
    "    wget.download(training_url, 'CONLL-2003') \n",
    "\n",
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "\n",
    "train_sents = list(ConllCorpusReader('CONLL-2003', 'eng.train', ['words', 'pos', 'ignore', 'chunk']).iob_sents())\n",
    "test_sents = list(ConllCorpusReader('CONLL-2003', 'eng.testb', ['words', 'pos', 'ignore', 'chunk']).iob_sents())\n",
    "validation_sents = list(ConllCorpusReader('CONLL-2003', 'eng.testa', ['words', 'pos', 'ignore', 'chunk']).iob_sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRv9wapygtVz"
   },
   "source": [
    "In the following cell we analyze our dataset so we know what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MFu0oXPkcSe3",
    "outputId": "d55712af-fd5c-4fef-ca88-1428e635e54a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the dataset:  301418\n",
      "Max. word count per sentence:  124\n",
      "Tags: ['I-ORG', 'I-MISC', 'B-LOC', 'O', 'I-PER', 'B-MISC', 'I-LOC', 'B-ORG']\n",
      "Number of Labels:  8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 227,\n",
       " 2: 1671,\n",
       " 3: 846,\n",
       " 4: 1155,\n",
       " 5: 1069,\n",
       " 6: 934,\n",
       " 7: 1542,\n",
       " 8: 1607,\n",
       " 9: 1247,\n",
       " 10: 753,\n",
       " 11: 591,\n",
       " 12: 470,\n",
       " 13: 465,\n",
       " 14: 410,\n",
       " 15: 378,\n",
       " 16: 330,\n",
       " 17: 331,\n",
       " 18: 304,\n",
       " 19: 297,\n",
       " 20: 335,\n",
       " 21: 350,\n",
       " 22: 321,\n",
       " 23: 308,\n",
       " 24: 309,\n",
       " 25: 321,\n",
       " 26: 306,\n",
       " 27: 285,\n",
       " 28: 309,\n",
       " 29: 324,\n",
       " 30: 292,\n",
       " 31: 302,\n",
       " 32: 263,\n",
       " 33: 257,\n",
       " 34: 224,\n",
       " 35: 196,\n",
       " 36: 193,\n",
       " 37: 169,\n",
       " 38: 151,\n",
       " 39: 148,\n",
       " 40: 114,\n",
       " 41: 103,\n",
       " 42: 93,\n",
       " 43: 81,\n",
       " 44: 71,\n",
       " 45: 62,\n",
       " 46: 38,\n",
       " 47: 28,\n",
       " 48: 25,\n",
       " 49: 22,\n",
       " 50: 21,\n",
       " 51: 13,\n",
       " 52: 16,\n",
       " 53: 9,\n",
       " 54: 6,\n",
       " 55: 12,\n",
       " 56: 6,\n",
       " 57: 3,\n",
       " 58: 2,\n",
       " 59: 4,\n",
       " 60: 3,\n",
       " 62: 3,\n",
       " 64: 1,\n",
       " 66: 1,\n",
       " 67: 1,\n",
       " 69: 1,\n",
       " 71: 1,\n",
       " 72: 1,\n",
       " 75: 1,\n",
       " 77: 1,\n",
       " 78: 1,\n",
       " 79: 1,\n",
       " 80: 1,\n",
       " 83: 1,\n",
       " 91: 1,\n",
       " 93: 1,\n",
       " 105: 1,\n",
       " 109: 1,\n",
       " 113: 1,\n",
       " 124: 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "max_count = 0\n",
    "\n",
    "tags = []\n",
    "words = []\n",
    "sentences = train_sents + test_sents + validation_sents\n",
    "\n",
    "lendict = {}\n",
    "count = 0\n",
    "\n",
    "for sent in sentences:\n",
    "    if count > 0:\n",
    "        try:\n",
    "            lendict[count] = lendict[count] + 1\n",
    "        except:\n",
    "            lendict[count] = 1\n",
    "    count = 0\n",
    "    \n",
    "    for entry in sent:\n",
    "        count = count+1\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            \n",
    "        words.append(entry[0])\n",
    "        tags.append(entry[2])\n",
    "\n",
    "tags = list(set(tags))\n",
    "\n",
    "n_words = len(words)\n",
    "print(\"Number of words in the dataset: \", n_words)\n",
    "print(\"Max. word count per sentence: \", max_count)\n",
    "\n",
    "print(\"Tags:\", tags)\n",
    "n_tags = len(tags)\n",
    "print(\"Number of Labels: \", n_tags)\n",
    "\n",
    "lendict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PYzpS_1HfOk"
   },
   "source": [
    "Now we need to prepare our dataset. In particular, we need to pad our word/tag sequences so we can feed them to the LSTM in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_nuJIjFyHfOk"
   },
   "outputs": [],
   "source": [
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1 # Unknown words\n",
    "word2idx[\"PAD\"] = 0 # Padding\n",
    "\n",
    "idx2word = {i: w for w, i in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9CaCUmsBHfOn"
   },
   "outputs": [],
   "source": [
    "tag2idx = {t: i+1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GuaxO-ZSHfOq"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_word_sequences(sentences):\n",
    "    X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "    X = pad_sequences(maxlen=MAX_LEN, sequences=X, padding=\"post\", value=0)\n",
    "    return X\n",
    "\n",
    "def prepare_tag_sequences(sentences):    \n",
    "    y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "    y = pad_sequences(maxlen=MAX_LEN, sequences=y, padding=\"post\", value=0)\n",
    "    y = np.array([to_categorical(i, num_classes=n_tags+1) for i in y])\n",
    "    return y\n",
    "\n",
    "X_training = prepare_word_sequences(train_sents)\n",
    "X_test = prepare_word_sequences(test_sents)\n",
    "X_validation = prepare_word_sequences(validation_sents)\n",
    "y_training = prepare_tag_sequences(train_sents)\n",
    "y_test = prepare_tag_sequences(test_sents)\n",
    "y_validation = prepare_tag_sequences(validation_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SP0eQt5iyUW9"
   },
   "source": [
    "Now we download the Glove embeddings so we can use them in the first layer of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EeVGTpr-OhFq",
    "outputId": "90d0c91d-1e5e-4a53-a392-a6933735ff0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONLL-2003  glove.6B.100d.txt  glove.6B.300d.txt  sample_data\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np\n",
    "\n",
    "if 'glove.6B.100d.txt' not in os.listdir():\n",
    "    target_url = 'https://www.dropbox.com/s/rft33p9cuynhp83/glove.6B.100d.txt?dl=1'\n",
    "    wget.download(target_url, '.')  \n",
    "\n",
    "if 'glove.6B.300d.txt' not in os.listdir():  \n",
    "    target_url = 'https://www.dropbox.com/s/gibuiv6mb7xgakp/glove.6B.300d.txt?dl=1'\n",
    "    wget.download(target_url, '.')   \n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnXMHOFhSgLt",
    "outputId": "626ad808-dbe6-44d4-b58f-2ec794cd116b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "if EMBEDDING == 100:\n",
    "    glove_file = 'glove.6B.100d.txt'\n",
    "else:\n",
    "    glove_file = 'glove.6B.300d.txt'\n",
    "\n",
    "f = open(glove_file)\n",
    "embeddings_index = {}\n",
    "\n",
    "for line in f:    \n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6ZVu7T2FOLVv"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((n_words+2, EMBEDDING))\n",
    "for i, word in enumerate(words):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5F4nxp9byl-u"
   },
   "source": [
    "And now we finally get to define our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofi5LXeaEjN5",
    "outputId": "dccedcba-775b-4cc3-d7fd-9c4e8f558dbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
      "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-nodqsnyl\n",
      "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-nodqsnyl\n",
      "Requirement already satisfied: keras in /tensorflow-1.15.2/python3.7 (from keras-contrib==2.0.8) (2.3.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.7 (from keras->keras-contrib==2.0.8) (1.0.8)\n",
      "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras->keras-contrib==2.0.8) (1.5.2)\n",
      "Building wheels for collected packages: keras-contrib\n",
      "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp37-none-any.whl size=101065 sha256=e7969e4cd3983428c5e5bed25babc52aafd078cb5a77a464838f5dc8fc657c01\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-73_b2fsx/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
      "Successfully built keras-contrib\n",
      "Installing collected packages: keras-contrib\n",
      "Successfully installed keras-contrib-2.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rpGcnUKDHfOu",
    "outputId": "f5c01f54-1944-4c01-930f-1f1634d8fd1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "/usr/local/lib/python3.7/dist-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_ops.py:2509: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 80, 300)           90426000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 80, 300)           541200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 80, 300)           0         \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 80, 9)             2808      \n",
      "=================================================================\n",
      "Total params: 90,970,008\n",
      "Trainable params: 90,970,008\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Masking\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "# Model definition\n",
    "inputs = Input(shape=(MAX_LEN,))\n",
    "model = Masking(mask_value=0, input_shape=(MAX_LEN,))(inputs)\n",
    "model = Embedding(input_dim=n_words + 2, # n_words + 2 (PAD & UNK)\n",
    "                  output_dim=EMBEDDING, \n",
    "                  weights=[embedding_matrix], \n",
    "                  trainable=True, \n",
    "                  input_length=MAX_LEN)(model)  \n",
    "model = Bidirectional(LSTM(units=LSTM_UNITS, \n",
    "                           return_sequences=True, \n",
    "                           recurrent_dropout=0.1, \n",
    "                           dropout=0.5))(model)  \n",
    "model = Dropout(rate=0.5)(model)\n",
    "#model = TimeDistributed(Dense(LSTM_UNITS, activation=\"relu\"))(model)  # some practitioners also add this layer\n",
    "\n",
    "crf = CRF(n_tags+1)  # CRF layer, n_tags+1 (PAD)\n",
    "out = crf(model)  # output\n",
    "\n",
    "model = Model(inputs, out)\n",
    "model.compile(optimizer=\"nadam\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jqJGnogHfOx"
   },
   "source": [
    "Now we evaluate our model. Every *CHECK_INTERVAL* epochs, we run it on the test set to see what's really happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cuNihtTGEjN6"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "ground_truth = np.argmax(y_test, -1)\n",
    "\n",
    "\n",
    "def IOB_classification_report(y_true, y_pred):\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O', 'PAD'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data, check_accuracy, check_performance):\n",
    "        self.test_data = test_data\n",
    "        self.accuracy = check_accuracy\n",
    "        self.run = check_performance\n",
    "\n",
    "    def on_epoch_end(self, epoch=0, logs={}):\n",
    "        x, y = self.test_data\n",
    "        if self.accuracy:\n",
    "            loss, acc = model.evaluate(x, y, verbose=1)\n",
    "            print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n",
    "        \n",
    "        if self.run or epoch % CHECK_INTERVAL == 0:\n",
    "            pred_cat = model.predict(x, verbose=1)\n",
    "            pred = np.argmax(pred_cat, axis=-1)\n",
    "\n",
    "            pred_tag = [[idx2tag[i] for i in row] for row in pred]\n",
    "            ground_truth_tag = [[idx2tag[i] for i in row] for row in ground_truth] \n",
    "\n",
    "            print(IOB_classification_report(ground_truth_tag, pred_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7Gtg4ofHfOx",
    "outputId": "cf724a02-14ca-4e9b-b898-4aabb26cda15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/100\n",
      "14987/14987 [==============================] - 15s 1ms/step - loss: 0.4418 - crf_viterbi_accuracy: 0.8511 - val_loss: 0.1233 - val_crf_viterbi_accuracy: 0.9660\n",
      "3684/3684 [==============================] - 10s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.00      0.00      0.00      1914\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "      I-MISC       0.00      0.00      0.00       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.00      0.00      0.00      2490\n",
      "       I-PER       0.61      0.03      0.05      2763\n",
      "\n",
      "   micro avg       0.56      0.01      0.02      8096\n",
      "   macro avg       0.09      0.00      0.01      8096\n",
      "weighted avg       0.21      0.01      0.02      8096\n",
      " samples avg       0.00      0.00      0.00      8096\n",
      "\n",
      "Epoch 2/100\n",
      "14987/14987 [==============================] - 12s 822us/step - loss: 0.1029 - crf_viterbi_accuracy: 0.9702 - val_loss: 0.0860 - val_crf_viterbi_accuracy: 0.9718\n",
      "Epoch 3/100\n",
      "14987/14987 [==============================] - 12s 822us/step - loss: 0.0652 - crf_viterbi_accuracy: 0.9791 - val_loss: 0.0535 - val_crf_viterbi_accuracy: 0.9826\n",
      "Epoch 4/100\n",
      "14987/14987 [==============================] - 12s 822us/step - loss: 0.0381 - crf_viterbi_accuracy: 0.9879 - val_loss: 0.0388 - val_crf_viterbi_accuracy: 0.9880\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.76      0.65      0.70      1914\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "      I-MISC       0.82      0.46      0.59       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.77      0.57      0.65      2490\n",
      "       I-PER       0.81      0.65      0.72      2763\n",
      "\n",
      "   micro avg       0.78      0.60      0.68      8096\n",
      "   macro avg       0.45      0.33      0.38      8096\n",
      "weighted avg       0.78      0.60      0.68      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 5/100\n",
      "14987/14987 [==============================] - 12s 827us/step - loss: 0.0235 - crf_viterbi_accuracy: 0.9928 - val_loss: 0.0323 - val_crf_viterbi_accuracy: 0.9902\n",
      "Epoch 6/100\n",
      "14987/14987 [==============================] - 12s 824us/step - loss: 0.0153 - crf_viterbi_accuracy: 0.9953 - val_loss: 0.0294 - val_crf_viterbi_accuracy: 0.9910\n",
      "Epoch 7/100\n",
      "14987/14987 [==============================] - 12s 823us/step - loss: 0.0107 - crf_viterbi_accuracy: 0.9965 - val_loss: 0.0281 - val_crf_viterbi_accuracy: 0.9916\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.85      0.77      0.81      1914\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "      I-MISC       0.72      0.69      0.71       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.80      0.67      0.73      2490\n",
      "       I-PER       0.86      0.72      0.78      2763\n",
      "\n",
      "   micro avg       0.82      0.71      0.76      8096\n",
      "   macro avg       0.46      0.41      0.43      8096\n",
      "weighted avg       0.82      0.71      0.76      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 8/100\n",
      "14987/14987 [==============================] - 12s 822us/step - loss: 0.0077 - crf_viterbi_accuracy: 0.9973 - val_loss: 0.0279 - val_crf_viterbi_accuracy: 0.9918\n",
      "Epoch 9/100\n",
      "14987/14987 [==============================] - 12s 824us/step - loss: 0.0059 - crf_viterbi_accuracy: 0.9978 - val_loss: 0.0285 - val_crf_viterbi_accuracy: 0.9919\n",
      "Epoch 10/100\n",
      "14987/14987 [==============================] - 12s 817us/step - loss: 0.0043 - crf_viterbi_accuracy: 0.9982 - val_loss: 0.0283 - val_crf_viterbi_accuracy: 0.9921\n",
      "3684/3684 [==============================] - 9s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.86      0.80      0.83      1914\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "      I-MISC       0.75      0.73      0.74       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.81      0.70      0.75      2490\n",
      "       I-PER       0.88      0.69      0.78      2763\n",
      "\n",
      "   micro avg       0.84      0.72      0.78      8096\n",
      "   macro avg       0.47      0.42      0.44      8096\n",
      "weighted avg       0.84      0.72      0.78      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 11/100\n",
      "14987/14987 [==============================] - 12s 822us/step - loss: 0.0035 - crf_viterbi_accuracy: 0.9984 - val_loss: 0.0285 - val_crf_viterbi_accuracy: 0.9922\n",
      "Epoch 12/100\n",
      "14987/14987 [==============================] - 12s 823us/step - loss: 0.0026 - crf_viterbi_accuracy: 0.9986 - val_loss: 0.0286 - val_crf_viterbi_accuracy: 0.9922\n",
      "Epoch 13/100\n",
      "14987/14987 [==============================] - 12s 824us/step - loss: 0.0018 - crf_viterbi_accuracy: 0.9988 - val_loss: 0.0282 - val_crf_viterbi_accuracy: 0.9924\n",
      "3684/3684 [==============================] - 9s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.87      0.81      0.84      1914\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "      I-MISC       0.75      0.73      0.74       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.81      0.72      0.76      2490\n",
      "       I-PER       0.88      0.70      0.78      2763\n",
      "\n",
      "   micro avg       0.84      0.74      0.78      8096\n",
      "   macro avg       0.47      0.42      0.45      8096\n",
      "weighted avg       0.84      0.74      0.78      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 14/100\n",
      "14987/14987 [==============================] - 12s 818us/step - loss: 0.0014 - crf_viterbi_accuracy: 0.9988 - val_loss: 0.0291 - val_crf_viterbi_accuracy: 0.9923\n",
      "Epoch 15/100\n",
      "14987/14987 [==============================] - 12s 822us/step - loss: 8.7100e-04 - crf_viterbi_accuracy: 0.9990 - val_loss: 0.0302 - val_crf_viterbi_accuracy: 0.9924\n",
      "Epoch 16/100\n",
      "14987/14987 [==============================] - 12s 822us/step - loss: 4.2827e-04 - crf_viterbi_accuracy: 0.9990 - val_loss: 0.0299 - val_crf_viterbi_accuracy: 0.9925\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.87      0.81      0.84      1914\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "      I-MISC       0.79      0.73      0.76       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.80      0.73      0.76      2490\n",
      "       I-PER       0.89      0.68      0.77      2763\n",
      "\n",
      "   micro avg       0.84      0.73      0.78      8096\n",
      "   macro avg       0.48      0.42      0.45      8096\n",
      "weighted avg       0.84      0.73      0.78      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 17/100\n",
      "14987/14987 [==============================] - 12s 825us/step - loss: -2.0868e-05 - crf_viterbi_accuracy: 0.9991 - val_loss: 0.0301 - val_crf_viterbi_accuracy: 0.9925\n",
      "Epoch 18/100\n",
      "14987/14987 [==============================] - 12s 820us/step - loss: -4.4442e-04 - crf_viterbi_accuracy: 0.9992 - val_loss: 0.0303 - val_crf_viterbi_accuracy: 0.9924\n",
      "Epoch 19/100\n",
      "14987/14987 [==============================] - 12s 819us/step - loss: -7.1933e-04 - crf_viterbi_accuracy: 0.9992 - val_loss: 0.0316 - val_crf_viterbi_accuracy: 0.9925\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.89      0.80      0.84      1914\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "      I-MISC       0.78      0.73      0.75       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.80      0.74      0.77      2490\n",
      "       I-PER       0.90      0.66      0.76      2763\n",
      "\n",
      "   micro avg       0.85      0.72      0.78      8096\n",
      "   macro avg       0.48      0.42      0.45      8096\n",
      "weighted avg       0.85      0.72      0.78      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 20/100\n",
      "14987/14987 [==============================] - 12s 826us/step - loss: -0.0011 - crf_viterbi_accuracy: 0.9993 - val_loss: 0.0313 - val_crf_viterbi_accuracy: 0.9925\n",
      "Epoch 21/100\n",
      "14987/14987 [==============================] - 12s 830us/step - loss: -0.0014 - crf_viterbi_accuracy: 0.9993 - val_loss: 0.0313 - val_crf_viterbi_accuracy: 0.9925\n",
      "Epoch 22/100\n",
      "14987/14987 [==============================] - 12s 825us/step - loss: -0.0018 - crf_viterbi_accuracy: 0.9993 - val_loss: 0.0318 - val_crf_viterbi_accuracy: 0.9926\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.88      0.81      0.84      1914\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "      I-MISC       0.77      0.73      0.75       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.82      0.73      0.77      2490\n",
      "       I-PER       0.89      0.67      0.76      2763\n",
      "\n",
      "   micro avg       0.85      0.73      0.78      8096\n",
      "   macro avg       0.48      0.42      0.45      8096\n",
      "weighted avg       0.85      0.73      0.78      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 23/100\n",
      "14987/14987 [==============================] - 12s 832us/step - loss: -0.0021 - crf_viterbi_accuracy: 0.9994 - val_loss: 0.0314 - val_crf_viterbi_accuracy: 0.9925\n",
      "Epoch 24/100\n",
      "14987/14987 [==============================] - 12s 825us/step - loss: -0.0024 - crf_viterbi_accuracy: 0.9994 - val_loss: 0.0317 - val_crf_viterbi_accuracy: 0.9925\n",
      "Epoch 25/100\n",
      "14987/14987 [==============================] - 12s 825us/step - loss: -0.0026 - crf_viterbi_accuracy: 0.9994 - val_loss: 0.0320 - val_crf_viterbi_accuracy: 0.9925\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.88      0.81      0.84      1914\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "      I-MISC       0.79      0.73      0.76       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.81      0.74      0.77      2490\n",
      "       I-PER       0.89      0.67      0.76      2763\n",
      "\n",
      "   micro avg       0.85      0.73      0.78      8096\n",
      "   macro avg       0.48      0.42      0.45      8096\n",
      "weighted avg       0.85      0.73      0.78      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 26/100\n",
      "14987/14987 [==============================] - 12s 831us/step - loss: -0.0030 - crf_viterbi_accuracy: 0.9995 - val_loss: 0.0324 - val_crf_viterbi_accuracy: 0.9925\n",
      "Epoch 27/100\n",
      "14987/14987 [==============================] - 12s 829us/step - loss: -0.0032 - crf_viterbi_accuracy: 0.9995 - val_loss: 0.0320 - val_crf_viterbi_accuracy: 0.9925\n",
      "Epoch 28/100\n",
      "14987/14987 [==============================] - 12s 828us/step - loss: -0.0036 - crf_viterbi_accuracy: 0.9995 - val_loss: 0.0311 - val_crf_viterbi_accuracy: 0.9925\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.87      0.82      0.84      1914\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "      I-MISC       0.75      0.74      0.75       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.83      0.73      0.78      2490\n",
      "       I-PER       0.88      0.69      0.77      2763\n",
      "\n",
      "   micro avg       0.85      0.73      0.79      8096\n",
      "   macro avg       0.48      0.42      0.45      8096\n",
      "weighted avg       0.85      0.73      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 29/100\n",
      "14987/14987 [==============================] - 13s 835us/step - loss: -0.0038 - crf_viterbi_accuracy: 0.9996 - val_loss: 0.0337 - val_crf_viterbi_accuracy: 0.9925\n",
      "Epoch 30/100\n",
      "14987/14987 [==============================] - 12s 829us/step - loss: -0.0042 - crf_viterbi_accuracy: 0.9996 - val_loss: 0.0340 - val_crf_viterbi_accuracy: 0.9925\n",
      "Epoch 31/100\n",
      "14987/14987 [==============================] - 12s 824us/step - loss: -0.0044 - crf_viterbi_accuracy: 0.9996 - val_loss: 0.0338 - val_crf_viterbi_accuracy: 0.9925\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.86      0.82      0.84      1914\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "      I-MISC       0.79      0.73      0.76       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.83      0.72      0.77      2490\n",
      "       I-PER       0.90      0.66      0.76      2763\n",
      "\n",
      "   micro avg       0.85      0.72      0.78      8096\n",
      "   macro avg       0.48      0.42      0.45      8096\n",
      "weighted avg       0.86      0.72      0.78      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 32/100\n",
      "14987/14987 [==============================] - 12s 828us/step - loss: -0.0047 - crf_viterbi_accuracy: 0.9996 - val_loss: 0.0349 - val_crf_viterbi_accuracy: 0.9925\n",
      "Epoch 33/100\n",
      "14987/14987 [==============================] - 12s 826us/step - loss: -0.0050 - crf_viterbi_accuracy: 0.9996 - val_loss: 0.0320 - val_crf_viterbi_accuracy: 0.9926\n",
      "Epoch 34/100\n",
      "14987/14987 [==============================] - 12s 827us/step - loss: -0.0053 - crf_viterbi_accuracy: 0.9997 - val_loss: 0.0339 - val_crf_viterbi_accuracy: 0.9925\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.85      0.82      0.84      1914\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "      I-MISC       0.78      0.73      0.75       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.86      0.70      0.77      2490\n",
      "       I-PER       0.89      0.67      0.77      2763\n",
      "\n",
      "   micro avg       0.86      0.72      0.78      8096\n",
      "   macro avg       0.48      0.42      0.45      8096\n",
      "weighted avg       0.86      0.72      0.78      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 35/100\n",
      "14987/14987 [==============================] - 12s 827us/step - loss: -0.0056 - crf_viterbi_accuracy: 0.9997 - val_loss: 0.0342 - val_crf_viterbi_accuracy: 0.9925\n",
      "Epoch 36/100\n",
      "14987/14987 [==============================] - 12s 832us/step - loss: -0.0059 - crf_viterbi_accuracy: 0.9997 - val_loss: 0.0343 - val_crf_viterbi_accuracy: 0.9925\n",
      "Epoch 37/100\n",
      "14987/14987 [==============================] - 12s 825us/step - loss: -0.0062 - crf_viterbi_accuracy: 0.9997 - val_loss: 0.0315 - val_crf_viterbi_accuracy: 0.9925\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.87      0.82      0.84      1914\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "      I-MISC       0.74      0.75      0.74       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.81      0.74      0.77      2490\n",
      "       I-PER       0.89      0.67      0.77      2763\n",
      "\n",
      "   micro avg       0.84      0.73      0.78      8096\n",
      "   macro avg       0.47      0.43      0.45      8096\n",
      "weighted avg       0.84      0.73      0.78      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 38/100\n",
      "14987/14987 [==============================] - 12s 831us/step - loss: -0.0065 - crf_viterbi_accuracy: 0.9997 - val_loss: 0.0342 - val_crf_viterbi_accuracy: 0.9925\n",
      "Epoch 39/100\n",
      "14987/14987 [==============================] - 12s 828us/step - loss: -0.0068 - crf_viterbi_accuracy: 0.9997 - val_loss: 0.0338 - val_crf_viterbi_accuracy: 0.9925\n",
      "Epoch 40/100\n",
      "14987/14987 [==============================] - 12s 832us/step - loss: -0.0071 - crf_viterbi_accuracy: 0.9997 - val_loss: 0.0331 - val_crf_viterbi_accuracy: 0.9926\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.85      0.83      0.84      1914\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "      I-MISC       0.77      0.74      0.75       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.86      0.70      0.77      2490\n",
      "       I-PER       0.90      0.67      0.77      2763\n",
      "\n",
      "   micro avg       0.86      0.72      0.78      8096\n",
      "   macro avg       0.48      0.42      0.45      8096\n",
      "weighted avg       0.86      0.72      0.78      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 41/100\n",
      "14987/14987 [==============================] - 12s 830us/step - loss: -0.0074 - crf_viterbi_accuracy: 0.9998 - val_loss: 0.0334 - val_crf_viterbi_accuracy: 0.9926\n",
      "Epoch 42/100\n",
      "14987/14987 [==============================] - 12s 831us/step - loss: -0.0077 - crf_viterbi_accuracy: 0.9998 - val_loss: 0.0344 - val_crf_viterbi_accuracy: 0.9925\n",
      "Epoch 43/100\n",
      "14987/14987 [==============================] - 12s 831us/step - loss: -0.0080 - crf_viterbi_accuracy: 0.9998 - val_loss: 0.0340 - val_crf_viterbi_accuracy: 0.9926\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.85      0.83      0.84      1914\n",
      "      B-MISC       0.33      0.11      0.17         9\n",
      "      I-MISC       0.79      0.73      0.76       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.87      0.70      0.78      2490\n",
      "       I-PER       0.91      0.67      0.77      2763\n",
      "\n",
      "   micro avg       0.87      0.73      0.79      8096\n",
      "   macro avg       0.54      0.44      0.47      8096\n",
      "weighted avg       0.87      0.73      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 44/100\n",
      "14987/14987 [==============================] - 13s 835us/step - loss: -0.0083 - crf_viterbi_accuracy: 0.9998 - val_loss: 0.0349 - val_crf_viterbi_accuracy: 0.9925\n",
      "Epoch 45/100\n",
      "14987/14987 [==============================] - 13s 837us/step - loss: -0.0087 - crf_viterbi_accuracy: 0.9998 - val_loss: 0.0349 - val_crf_viterbi_accuracy: 0.9926\n",
      "Epoch 46/100\n",
      "14987/14987 [==============================] - 13s 836us/step - loss: -0.0089 - crf_viterbi_accuracy: 0.9998 - val_loss: 0.0334 - val_crf_viterbi_accuracy: 0.9926\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.86      0.84      0.85      1914\n",
      "      B-MISC       0.33      0.11      0.17         9\n",
      "      I-MISC       0.78      0.74      0.76       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.84      0.72      0.77      2490\n",
      "       I-PER       0.90      0.67      0.77      2763\n",
      "\n",
      "   micro avg       0.86      0.73      0.79      8096\n",
      "   macro avg       0.53      0.44      0.47      8096\n",
      "weighted avg       0.86      0.73      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 47/100\n",
      "14987/14987 [==============================] - 13s 837us/step - loss: -0.0092 - crf_viterbi_accuracy: 0.9998 - val_loss: 0.0343 - val_crf_viterbi_accuracy: 0.9926\n",
      "Epoch 48/100\n",
      "14987/14987 [==============================] - 13s 838us/step - loss: -0.0096 - crf_viterbi_accuracy: 0.9998 - val_loss: 0.0350 - val_crf_viterbi_accuracy: 0.9926\n",
      "Epoch 49/100\n",
      "14987/14987 [==============================] - 13s 838us/step - loss: -0.0099 - crf_viterbi_accuracy: 0.9998 - val_loss: 0.0354 - val_crf_viterbi_accuracy: 0.9926\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.88      0.82      0.84      1914\n",
      "      B-MISC       0.33      0.11      0.17         9\n",
      "      I-MISC       0.81      0.72      0.76       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.84      0.72      0.78      2490\n",
      "       I-PER       0.91      0.66      0.77      2763\n",
      "\n",
      "   micro avg       0.87      0.72      0.79      8096\n",
      "   macro avg       0.54      0.43      0.47      8096\n",
      "weighted avg       0.87      0.72      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 50/100\n",
      "14987/14987 [==============================] - 12s 832us/step - loss: -0.0102 - crf_viterbi_accuracy: 0.9998 - val_loss: 0.0344 - val_crf_viterbi_accuracy: 0.9926\n",
      "Epoch 51/100\n",
      "14987/14987 [==============================] - 12s 830us/step - loss: -0.0106 - crf_viterbi_accuracy: 0.9998 - val_loss: 0.0337 - val_crf_viterbi_accuracy: 0.9927\n",
      "Epoch 52/100\n",
      "14987/14987 [==============================] - 12s 831us/step - loss: -0.0109 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0338 - val_crf_viterbi_accuracy: 0.9926\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.88      0.81      0.85      1914\n",
      "      B-MISC       0.33      0.11      0.17         9\n",
      "      I-MISC       0.78      0.73      0.75       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.83      0.73      0.78      2490\n",
      "       I-PER       0.91      0.67      0.77      2763\n",
      "\n",
      "   micro avg       0.86      0.73      0.79      8096\n",
      "   macro avg       0.53      0.44      0.47      8096\n",
      "weighted avg       0.86      0.73      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 53/100\n",
      "14987/14987 [==============================] - 12s 832us/step - loss: -0.0113 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0334 - val_crf_viterbi_accuracy: 0.9927\n",
      "Epoch 54/100\n",
      "14987/14987 [==============================] - 12s 826us/step - loss: -0.0115 - crf_viterbi_accuracy: 0.9998 - val_loss: 0.0344 - val_crf_viterbi_accuracy: 0.9927\n",
      "Epoch 55/100\n",
      "14987/14987 [==============================] - 12s 830us/step - loss: -0.0118 - crf_viterbi_accuracy: 0.9998 - val_loss: 0.0342 - val_crf_viterbi_accuracy: 0.9926\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.87      0.82      0.84      1914\n",
      "      B-MISC       0.25      0.11      0.15         9\n",
      "      I-MISC       0.77      0.73      0.75       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.85      0.72      0.78      2490\n",
      "       I-PER       0.91      0.67      0.77      2763\n",
      "\n",
      "   micro avg       0.86      0.73      0.79      8096\n",
      "   macro avg       0.52      0.44      0.47      8096\n",
      "weighted avg       0.86      0.73      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 56/100\n",
      "14987/14987 [==============================] - 13s 835us/step - loss: -0.0123 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0334 - val_crf_viterbi_accuracy: 0.9927\n",
      "Epoch 57/100\n",
      "14987/14987 [==============================] - 12s 827us/step - loss: -0.0126 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0350 - val_crf_viterbi_accuracy: 0.9926\n",
      "Epoch 58/100\n",
      "14987/14987 [==============================] - 12s 832us/step - loss: -0.0130 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0353 - val_crf_viterbi_accuracy: 0.9926\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.87      0.82      0.85      1914\n",
      "      B-MISC       0.25      0.11      0.15         9\n",
      "      I-MISC       0.80      0.73      0.76       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.85      0.72      0.78      2490\n",
      "       I-PER       0.91      0.66      0.76      2763\n",
      "\n",
      "   micro avg       0.87      0.72      0.79      8096\n",
      "   macro avg       0.53      0.43      0.47      8096\n",
      "weighted avg       0.87      0.72      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 59/100\n",
      "14987/14987 [==============================] - 12s 831us/step - loss: -0.0134 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0330 - val_crf_viterbi_accuracy: 0.9925\n",
      "Epoch 60/100\n",
      "14987/14987 [==============================] - 12s 829us/step - loss: -0.0137 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0318 - val_crf_viterbi_accuracy: 0.9927\n",
      "Epoch 61/100\n",
      "14987/14987 [==============================] - 12s 831us/step - loss: -0.0140 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0326 - val_crf_viterbi_accuracy: 0.9926\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.87      0.83      0.85      1914\n",
      "      B-MISC       0.25      0.11      0.15         9\n",
      "      I-MISC       0.78      0.73      0.76       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.83      0.73      0.78      2490\n",
      "       I-PER       0.91      0.67      0.77      2763\n",
      "\n",
      "   micro avg       0.86      0.73      0.79      8096\n",
      "   macro avg       0.52      0.44      0.47      8096\n",
      "weighted avg       0.86      0.73      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 62/100\n",
      "14987/14987 [==============================] - 12s 833us/step - loss: -0.0145 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0327 - val_crf_viterbi_accuracy: 0.9926\n",
      "Epoch 63/100\n",
      "14987/14987 [==============================] - 12s 833us/step - loss: -0.0148 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0319 - val_crf_viterbi_accuracy: 0.9927\n",
      "Epoch 64/100\n",
      "14987/14987 [==============================] - 12s 830us/step - loss: -0.0152 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0354 - val_crf_viterbi_accuracy: 0.9926\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.88      0.82      0.84      1914\n",
      "      B-MISC       0.20      0.11      0.14         9\n",
      "      I-MISC       0.79      0.72      0.75       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.86      0.72      0.78      2490\n",
      "       I-PER       0.92      0.64      0.76      2763\n",
      "\n",
      "   micro avg       0.87      0.71      0.79      8096\n",
      "   macro avg       0.52      0.43      0.47      8096\n",
      "weighted avg       0.87      0.71      0.78      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 65/100\n",
      "14987/14987 [==============================] - 12s 828us/step - loss: -0.0156 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0292 - val_crf_viterbi_accuracy: 0.9928\n",
      "Epoch 66/100\n",
      "14987/14987 [==============================] - 12s 831us/step - loss: -0.0160 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0322 - val_crf_viterbi_accuracy: 0.9926\n",
      "Epoch 67/100\n",
      "14987/14987 [==============================] - 12s 831us/step - loss: -0.0164 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0331 - val_crf_viterbi_accuracy: 0.9926\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.87      0.82      0.84      1914\n",
      "      B-MISC       0.20      0.11      0.14         9\n",
      "      I-MISC       0.80      0.73      0.76       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.85      0.72      0.78      2490\n",
      "       I-PER       0.91      0.66      0.77      2763\n",
      "\n",
      "   micro avg       0.87      0.72      0.79      8096\n",
      "   macro avg       0.52      0.43      0.47      8096\n",
      "weighted avg       0.87      0.72      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 68/100\n",
      "14987/14987 [==============================] - 13s 840us/step - loss: -0.0168 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0332 - val_crf_viterbi_accuracy: 0.9927\n",
      "Epoch 69/100\n",
      "14987/14987 [==============================] - 13s 840us/step - loss: -0.0172 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0322 - val_crf_viterbi_accuracy: 0.9926\n",
      "Epoch 70/100\n",
      "14987/14987 [==============================] - 13s 838us/step - loss: -0.0176 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0320 - val_crf_viterbi_accuracy: 0.9927\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.88      0.82      0.85      1914\n",
      "      B-MISC       0.20      0.11      0.14         9\n",
      "      I-MISC       0.79      0.73      0.76       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.84      0.73      0.79      2490\n",
      "       I-PER       0.91      0.66      0.77      2763\n",
      "\n",
      "   micro avg       0.87      0.73      0.79      8096\n",
      "   macro avg       0.52      0.44      0.47      8096\n",
      "weighted avg       0.87      0.73      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 71/100\n",
      "14987/14987 [==============================] - 13s 836us/step - loss: -0.0180 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0330 - val_crf_viterbi_accuracy: 0.9926\n",
      "Epoch 72/100\n",
      "14987/14987 [==============================] - 13s 839us/step - loss: -0.0185 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0326 - val_crf_viterbi_accuracy: 0.9926\n",
      "Epoch 73/100\n",
      "14987/14987 [==============================] - 13s 841us/step - loss: -0.0189 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0293 - val_crf_viterbi_accuracy: 0.9927\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.87      0.83      0.85      1914\n",
      "      B-MISC       0.20      0.11      0.14         9\n",
      "      I-MISC       0.78      0.72      0.75       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.84      0.72      0.78      2490\n",
      "       I-PER       0.90      0.68      0.78      2763\n",
      "\n",
      "   micro avg       0.86      0.73      0.79      8096\n",
      "   macro avg       0.51      0.44      0.47      8096\n",
      "weighted avg       0.86      0.73      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 74/100\n",
      "14987/14987 [==============================] - 13s 836us/step - loss: -0.0193 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0299 - val_crf_viterbi_accuracy: 0.9927\n",
      "Epoch 75/100\n",
      "14987/14987 [==============================] - 12s 834us/step - loss: -0.0198 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0291 - val_crf_viterbi_accuracy: 0.9928\n",
      "Epoch 76/100\n",
      "14987/14987 [==============================] - 12s 833us/step - loss: -0.0202 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0290 - val_crf_viterbi_accuracy: 0.9928\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.88      0.82      0.85      1914\n",
      "      B-MISC       0.25      0.11      0.15         9\n",
      "      I-MISC       0.78      0.72      0.75       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.83      0.73      0.78      2490\n",
      "       I-PER       0.90      0.68      0.78      2763\n",
      "\n",
      "   micro avg       0.86      0.73      0.79      8096\n",
      "   macro avg       0.52      0.44      0.47      8096\n",
      "weighted avg       0.86      0.73      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 77/100\n",
      "14987/14987 [==============================] - 13s 834us/step - loss: -0.0207 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0287 - val_crf_viterbi_accuracy: 0.9928\n",
      "Epoch 78/100\n",
      "14987/14987 [==============================] - 13s 838us/step - loss: -0.0211 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0302 - val_crf_viterbi_accuracy: 0.9927\n",
      "Epoch 79/100\n",
      "14987/14987 [==============================] - 13s 842us/step - loss: -0.0216 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0289 - val_crf_viterbi_accuracy: 0.9928\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.89      0.81      0.85      1914\n",
      "      B-MISC       0.20      0.11      0.14         9\n",
      "      I-MISC       0.78      0.73      0.75       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.85      0.72      0.78      2490\n",
      "       I-PER       0.90      0.68      0.78      2763\n",
      "\n",
      "   micro avg       0.86      0.73      0.79      8096\n",
      "   macro avg       0.52      0.44      0.47      8096\n",
      "weighted avg       0.86      0.73      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 80/100\n",
      "14987/14987 [==============================] - 13s 839us/step - loss: -0.0220 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0295 - val_crf_viterbi_accuracy: 0.9927\n",
      "Epoch 81/100\n",
      "14987/14987 [==============================] - 13s 836us/step - loss: -0.0225 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0269 - val_crf_viterbi_accuracy: 0.9927\n",
      "Epoch 82/100\n",
      "14987/14987 [==============================] - 13s 839us/step - loss: -0.0229 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0288 - val_crf_viterbi_accuracy: 0.9927\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.88      0.82      0.85      1914\n",
      "      B-MISC       0.20      0.11      0.14         9\n",
      "      I-MISC       0.78      0.73      0.76       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.83      0.73      0.78      2490\n",
      "       I-PER       0.91      0.66      0.77      2763\n",
      "\n",
      "   micro avg       0.86      0.73      0.79      8096\n",
      "   macro avg       0.52      0.44      0.47      8096\n",
      "weighted avg       0.86      0.73      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 83/100\n",
      "14987/14987 [==============================] - 12s 834us/step - loss: -0.0234 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0272 - val_crf_viterbi_accuracy: 0.9928\n",
      "Epoch 84/100\n",
      "14987/14987 [==============================] - 12s 831us/step - loss: -0.0239 - crf_viterbi_accuracy: 1.0000 - val_loss: 0.0264 - val_crf_viterbi_accuracy: 0.9928\n",
      "Epoch 85/100\n",
      "14987/14987 [==============================] - 12s 830us/step - loss: -0.0244 - crf_viterbi_accuracy: 1.0000 - val_loss: 0.0279 - val_crf_viterbi_accuracy: 0.9927\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.87      0.83      0.85      1914\n",
      "      B-MISC       0.17      0.11      0.13         9\n",
      "      I-MISC       0.77      0.73      0.75       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.86      0.72      0.78      2490\n",
      "       I-PER       0.91      0.67      0.77      2763\n",
      "\n",
      "   micro avg       0.87      0.73      0.79      8096\n",
      "   macro avg       0.51      0.44      0.47      8096\n",
      "weighted avg       0.87      0.73      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 86/100\n",
      "14987/14987 [==============================] - 12s 833us/step - loss: -0.0249 - crf_viterbi_accuracy: 1.0000 - val_loss: 0.0292 - val_crf_viterbi_accuracy: 0.9926\n",
      "Epoch 87/100\n",
      "14987/14987 [==============================] - 13s 841us/step - loss: -0.0253 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0270 - val_crf_viterbi_accuracy: 0.9927\n",
      "Epoch 88/100\n",
      "14987/14987 [==============================] - 12s 830us/step - loss: -0.0258 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0257 - val_crf_viterbi_accuracy: 0.9927\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.86      0.84      0.85      1914\n",
      "      B-MISC       0.17      0.11      0.13         9\n",
      "      I-MISC       0.76      0.74      0.75       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.85      0.72      0.78      2490\n",
      "       I-PER       0.92      0.66      0.77      2763\n",
      "\n",
      "   micro avg       0.86      0.73      0.79      8096\n",
      "   macro avg       0.51      0.44      0.47      8096\n",
      "weighted avg       0.86      0.73      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 89/100\n",
      "14987/14987 [==============================] - 12s 832us/step - loss: -0.0264 - crf_viterbi_accuracy: 1.0000 - val_loss: 0.0256 - val_crf_viterbi_accuracy: 0.9928\n",
      "Epoch 90/100\n",
      "14987/14987 [==============================] - 12s 829us/step - loss: -0.0268 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0249 - val_crf_viterbi_accuracy: 0.9927\n",
      "Epoch 91/100\n",
      "14987/14987 [==============================] - 12s 831us/step - loss: -0.0273 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0269 - val_crf_viterbi_accuracy: 0.9927\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.88      0.82      0.85      1914\n",
      "      B-MISC       0.17      0.11      0.13         9\n",
      "      I-MISC       0.78      0.72      0.75       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.84      0.72      0.78      2490\n",
      "       I-PER       0.91      0.66      0.77      2763\n",
      "\n",
      "   micro avg       0.86      0.72      0.79      8096\n",
      "   macro avg       0.51      0.43      0.47      8096\n",
      "weighted avg       0.87      0.72      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 92/100\n",
      "14987/14987 [==============================] - 12s 833us/step - loss: -0.0279 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0245 - val_crf_viterbi_accuracy: 0.9927\n",
      "Epoch 93/100\n",
      "14987/14987 [==============================] - 12s 833us/step - loss: -0.0284 - crf_viterbi_accuracy: 1.0000 - val_loss: 0.0240 - val_crf_viterbi_accuracy: 0.9927\n",
      "Epoch 94/100\n",
      "14987/14987 [==============================] - 12s 832us/step - loss: -0.0289 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0235 - val_crf_viterbi_accuracy: 0.9927\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.87      0.84      0.85      1914\n",
      "      B-MISC       0.17      0.11      0.13         9\n",
      "      I-MISC       0.78      0.73      0.76       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.84      0.73      0.78      2490\n",
      "       I-PER       0.91      0.67      0.77      2763\n",
      "\n",
      "   micro avg       0.86      0.73      0.79      8096\n",
      "   macro avg       0.51      0.44      0.47      8096\n",
      "weighted avg       0.86      0.73      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 95/100\n",
      "14987/14987 [==============================] - 12s 830us/step - loss: -0.0294 - crf_viterbi_accuracy: 1.0000 - val_loss: 0.0257 - val_crf_viterbi_accuracy: 0.9926\n",
      "Epoch 96/100\n",
      "14987/14987 [==============================] - 12s 831us/step - loss: -0.0299 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0240 - val_crf_viterbi_accuracy: 0.9927\n",
      "Epoch 97/100\n",
      "14987/14987 [==============================] - 12s 831us/step - loss: -0.0305 - crf_viterbi_accuracy: 1.0000 - val_loss: 0.0234 - val_crf_viterbi_accuracy: 0.9927\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.87      0.82      0.85      1914\n",
      "      B-MISC       0.14      0.11      0.12         9\n",
      "      I-MISC       0.79      0.73      0.76       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.84      0.72      0.78      2490\n",
      "       I-PER       0.91      0.67      0.77      2763\n",
      "\n",
      "   micro avg       0.86      0.73      0.79      8096\n",
      "   macro avg       0.51      0.44      0.47      8096\n",
      "weighted avg       0.86      0.73      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n",
      "Epoch 98/100\n",
      "14987/14987 [==============================] - 12s 833us/step - loss: -0.0310 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0221 - val_crf_viterbi_accuracy: 0.9927\n",
      "Epoch 99/100\n",
      "14987/14987 [==============================] - 13s 839us/step - loss: -0.0315 - crf_viterbi_accuracy: 1.0000 - val_loss: 0.0232 - val_crf_viterbi_accuracy: 0.9927\n",
      "Epoch 100/100\n",
      "14987/14987 [==============================] - 12s 827us/step - loss: -0.0321 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0240 - val_crf_viterbi_accuracy: 0.9927\n",
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.88      0.82      0.85      1914\n",
      "      B-MISC       0.17      0.11      0.13         9\n",
      "      I-MISC       0.79      0.72      0.76       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.85      0.72      0.78      2490\n",
      "       I-PER       0.91      0.67      0.77      2763\n",
      "\n",
      "   micro avg       0.87      0.72      0.79      8096\n",
      "   macro avg       0.51      0.43      0.47      8096\n",
      "weighted avg       0.87      0.72      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_accuracy = False\n",
    "check_performance = False\n",
    "CHECK_INTERVAL = 3\n",
    "\n",
    "history = model.fit(X_training, y_training, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=EPOCHS, \n",
    "                    validation_data=(X_validation, y_validation), \n",
    "                    shuffle=True, \n",
    "                    verbose=1,\n",
    "                    callbacks=[TestCallback((X_test, y_test), check_accuracy, check_performance)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "i86dthvYN09U",
    "outputId": "c8b2ae36-2b50-41e9-cae2-24f99fe0abe5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "ZeZKMaY1OKf3",
    "outputId": "ce99137f-b41a-4f84-eac1-a8eb89b24430"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['crf_viterbi_accuracy'], label='crf_viterbi_accuracy')\n",
    "plt.plot(history.history['val_crf_viterbi_accuracy'], label='val_crf_viterbi_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WizknWuOoEAI",
    "outputId": "1d080f39-f65f-45c9-f72e-b23db612026e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684/3684 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "       I-LOC       0.88      0.82      0.85      1914\n",
      "      B-MISC       0.17      0.11      0.13         9\n",
      "      I-MISC       0.79      0.72      0.76       909\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-ORG       0.85      0.72      0.78      2490\n",
      "       I-PER       0.91      0.67      0.77      2763\n",
      "\n",
      "   micro avg       0.87      0.72      0.79      8096\n",
      "   macro avg       0.51      0.43      0.47      8096\n",
      "weighted avg       0.87      0.72      0.79      8096\n",
      " samples avg       0.02      0.02      0.02      8096\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_cat = model.predict(X_test, verbose=1)\n",
    "pred = np.argmax(pred_cat, axis=-1)\n",
    "\n",
    "pred_tag = [[idx2tag[i] for i in row] for row in pred]\n",
    "ground_truth_tag = [[idx2tag[i] for i in row] for row in ground_truth] \n",
    "\n",
    "print (IOB_classification_report(ground_truth_tag, pred_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzZLJiaAE1ja"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_NER.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
