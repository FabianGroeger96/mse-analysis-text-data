{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://www.msengineering.ch/typo3conf/ext/nm_theme_msengineering/Resources/Public/Template/img/mse_logo.jpg \"MSE Logo\") \n",
    "\n",
    "# AnTeDe Lab 2: Text Classification - Part A\n",
    "\n",
    "## Session goal\n",
    "The goal of this session is to implement a Multinomial Naive Bayes classifier from scratch.\n",
    "\n",
    "## Data collection\n",
    "We are going to use a small toy dataset. Each document is a single sentence. The training data contains three documents, each from a different class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Daniele/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/Daniele/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Daniele/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# these 3 lines are here for compatibility purposes\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "#\n",
    "\n",
    "training_corpus=[\"The Limmat flows out of the lake.\", \n",
    "           \"The bears are in the bear pit near the river.\",\n",
    "           \"The Rhône flows out of Lake Geneva.\",\n",
    "          ]\n",
    "training_labels=[\"zurich\", \n",
    "         \"bern\",\n",
    "         \"geneva\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to need a helper function that can normalize a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(document, keep_punctuation=False, \\\n",
    "                  keep_stop_words=False, keep_inflected=True, keep_numbers=False):\n",
    "            import string\n",
    "            \n",
    "            from nltk.corpus import stopwords\n",
    "            from nltk.stem import WordNetLemmatizer\n",
    "            from nltk.tokenize import word_tokenize\n",
    "\n",
    "             \n",
    "            word_tokens = word_tokenize(document)\n",
    "\n",
    "            wl = WordNetLemmatizer()\n",
    "            lemmatize = lambda tokens: \\\n",
    "                [wl.lemmatize(w) for w in tokens]\n",
    "\n",
    "\n",
    "            stop_words=set(stopwords.words('english'))\n",
    "            normalized = [w.lower() for w in word_tokens \n",
    "                               if ((not w.lower() in set(string.punctuation)) \\\n",
    "                                   or keep_punctuation)\n",
    "                               and\n",
    "                               ((not w.lower() in stop_words) or keep_stop_words)\n",
    "                               and\n",
    "                               ((w.lower().isalnum()) or keep_punctuation)\n",
    "                               and\n",
    "                               (not (w.lower().isdigit()) or keep_numbers)\n",
    "                               ] \n",
    "\n",
    "            if keep_inflected is False:\n",
    "                normalized = lemmatize(normalized)\n",
    "            \n",
    "            return normalized "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does *keep_inflected* affect the output of __normalize__?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        original                      normalized                        inflected\n",
      "0              The Limmat flows out of the lake.            [limmat, flow, lake]            [limmat, flows, lake]\n",
      "1  The bears are in the bear pit near the river.  [bear, bear, pit, near, river]  [bears, bear, pit, near, river]\n",
      "2            The Rhône flows out of Lake Geneva.     [rhône, flow, lake, geneva]     [rhône, flows, lake, geneva]\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_REMOVE\n",
    "normalized_training_corpus = [normalize(item, keep_inflected=False) for item in training_corpus]    \n",
    "inflected_training_corpus = [normalize(item, keep_inflected=True) for item in training_corpus] \n",
    "\n",
    "df=pd.DataFrame(columns=['original', 'normalized', 'inflected'])\n",
    "df['original']=training_corpus\n",
    "df['normalized']=normalized_training_corpus\n",
    "df['inflected']=inflected_training_corpus\n",
    "print (df.to_string())\n",
    "# keep_inflected maintains inflected forms such as 'cities'\n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now, we need to define a __get_vocabulary__ function that gets us all the unique words that appear in the normalized documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary (data):\n",
    "    return list(set(sum(data,[])))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flow', 'lake', 'geneva', 'bear', 'rhône', 'pit', 'river', 'limmat', 'near']\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_REMOVE\n",
    "print(get_vocabulary(normalized_training_corpus))  \n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a class __ms_timer__ that helps us time snippets of code. Its definition follows a special syntax that serves to implement what is known as a context manager. \n",
    "\n",
    "Each code snippet that we wish to time will be placed in an indented block following a __with__ statement. At the end of the indented block, the run time of the snippet will be returned by the class method __get_elapsed_time__. \n",
    "\n",
    "(You can do this same thing effortlessly in an IDE with profiling, but this is a good way to do it in a Jupyter notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class ms_timer:\n",
    "            \n",
    "    def __enter__(self):\n",
    "        self.start=time.time()\n",
    "        return self\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.stop=time.time()\n",
    "    def get_elapsed_time(self):\n",
    "        return 1000*(self.stop-self.start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of how to time code snippets using the context manager trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for the loop: 0.0041 ms\n"
     ]
    }
   ],
   "source": [
    "my_data = range(1, 10)\n",
    "\n",
    "with ms_timer() as timer:\n",
    "    prod=1\n",
    "    for item in my_data:\n",
    "        prod=prod*item\n",
    "print (\"Elapsed time for the loop: \"+str(round(timer.get_elapsed_time(), 4))+\" ms\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNB from scratch\n",
    "\n",
    "We are now ready to implement our MNB from scratch. Our implementation is contained in a class called __naive_bayes__. We can define our class across multiple cells simply by defining a derived class with exactly the same name in the following cells.\n",
    "\n",
    "First we compute the posterior probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class naive_bayes:\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_posterior_probabilities (training_data, verbose=False):\n",
    "\n",
    "        import logging, re\n",
    "\n",
    "        posterior = {}\n",
    "\n",
    "        vocabulary = get_vocabulary(training_data['documents'])\n",
    "        lw = len(vocabulary)\n",
    "\n",
    "        classes = list(set(training_labels))\n",
    "\n",
    "        for index, c in enumerate(classes): \n",
    "\n",
    "            tokens = sum(training_data['documents']\\\n",
    "                                         [training_data['labels']==c], [])\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                den = len(tokens)    \n",
    "            except:\n",
    "                den=0\n",
    "\n",
    "            current_class_docs = tokens\n",
    "\n",
    "            for w in vocabulary:\n",
    "\n",
    "                num = current_class_docs.count(w)                \n",
    "                posterior[(w,c)]=(1+num)/(den+lw)\n",
    "\n",
    "                if verbose:\n",
    "                    \n",
    "                    message = 'Token '+w+' appears '+str(num)+' times in class '+c\n",
    "                    message=re.sub('1 times', 'once', message)\n",
    "                    logging.warning (message)\n",
    "                    \n",
    "                    message = 'There are '+str(den)+' tokens in class '+c\n",
    "                    message=re.sub('are 1 tokens', 'is 1 token', message)\n",
    "                    logging.warning (message)\n",
    "                    \n",
    "                    logging.warning (current_class_docs)\n",
    "                    logging.warning ('Vocab size: '+str(lw))\n",
    "                    logging.warning ('Vanilla posterior: '+str(round(num/den, 2)))\n",
    "                    logging.warning ('Posterior: '+str(round(posterior[(w,c)], 2)))\n",
    "\n",
    "        return posterior  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) What is the posterior probability of finding 'limmat' given that the document is tagged as 'zurich'? Complete the following code snippet to find out. Use *verbose* to see what's going on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Token flow appears 0 times in class bern\n",
      "WARNING:root:There are 5 tokens in class bern\n",
      "WARNING:root:['bear', 'bear', 'pit', 'near', 'river']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.0\n",
      "WARNING:root:Posterior: 0.07\n",
      "WARNING:root:Token lake appears 0 times in class bern\n",
      "WARNING:root:There are 5 tokens in class bern\n",
      "WARNING:root:['bear', 'bear', 'pit', 'near', 'river']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.0\n",
      "WARNING:root:Posterior: 0.07\n",
      "WARNING:root:Token geneva appears 0 times in class bern\n",
      "WARNING:root:There are 5 tokens in class bern\n",
      "WARNING:root:['bear', 'bear', 'pit', 'near', 'river']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.0\n",
      "WARNING:root:Posterior: 0.07\n",
      "WARNING:root:Token bear appears 2 times in class bern\n",
      "WARNING:root:There are 5 tokens in class bern\n",
      "WARNING:root:['bear', 'bear', 'pit', 'near', 'river']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.4\n",
      "WARNING:root:Posterior: 0.21\n",
      "WARNING:root:Token rhône appears 0 times in class bern\n",
      "WARNING:root:There are 5 tokens in class bern\n",
      "WARNING:root:['bear', 'bear', 'pit', 'near', 'river']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.0\n",
      "WARNING:root:Posterior: 0.07\n",
      "WARNING:root:Token pit appears once in class bern\n",
      "WARNING:root:There are 5 tokens in class bern\n",
      "WARNING:root:['bear', 'bear', 'pit', 'near', 'river']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.2\n",
      "WARNING:root:Posterior: 0.14\n",
      "WARNING:root:Token river appears once in class bern\n",
      "WARNING:root:There are 5 tokens in class bern\n",
      "WARNING:root:['bear', 'bear', 'pit', 'near', 'river']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.2\n",
      "WARNING:root:Posterior: 0.14\n",
      "WARNING:root:Token limmat appears 0 times in class bern\n",
      "WARNING:root:There are 5 tokens in class bern\n",
      "WARNING:root:['bear', 'bear', 'pit', 'near', 'river']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.0\n",
      "WARNING:root:Posterior: 0.07\n",
      "WARNING:root:Token near appears once in class bern\n",
      "WARNING:root:There are 5 tokens in class bern\n",
      "WARNING:root:['bear', 'bear', 'pit', 'near', 'river']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.2\n",
      "WARNING:root:Posterior: 0.14\n",
      "WARNING:root:Token flow appears once in class zurich\n",
      "WARNING:root:There are 3 tokens in class zurich\n",
      "WARNING:root:['limmat', 'flow', 'lake']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.33\n",
      "WARNING:root:Posterior: 0.17\n",
      "WARNING:root:Token lake appears once in class zurich\n",
      "WARNING:root:There are 3 tokens in class zurich\n",
      "WARNING:root:['limmat', 'flow', 'lake']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.33\n",
      "WARNING:root:Posterior: 0.17\n",
      "WARNING:root:Token geneva appears 0 times in class zurich\n",
      "WARNING:root:There are 3 tokens in class zurich\n",
      "WARNING:root:['limmat', 'flow', 'lake']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.0\n",
      "WARNING:root:Posterior: 0.08\n",
      "WARNING:root:Token bear appears 0 times in class zurich\n",
      "WARNING:root:There are 3 tokens in class zurich\n",
      "WARNING:root:['limmat', 'flow', 'lake']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.0\n",
      "WARNING:root:Posterior: 0.08\n",
      "WARNING:root:Token rhône appears 0 times in class zurich\n",
      "WARNING:root:There are 3 tokens in class zurich\n",
      "WARNING:root:['limmat', 'flow', 'lake']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.0\n",
      "WARNING:root:Posterior: 0.08\n",
      "WARNING:root:Token pit appears 0 times in class zurich\n",
      "WARNING:root:There are 3 tokens in class zurich\n",
      "WARNING:root:['limmat', 'flow', 'lake']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.0\n",
      "WARNING:root:Posterior: 0.08\n",
      "WARNING:root:Token river appears 0 times in class zurich\n",
      "WARNING:root:There are 3 tokens in class zurich\n",
      "WARNING:root:['limmat', 'flow', 'lake']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.0\n",
      "WARNING:root:Posterior: 0.08\n",
      "WARNING:root:Token limmat appears once in class zurich\n",
      "WARNING:root:There are 3 tokens in class zurich\n",
      "WARNING:root:['limmat', 'flow', 'lake']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.33\n",
      "WARNING:root:Posterior: 0.17\n",
      "WARNING:root:Token near appears 0 times in class zurich\n",
      "WARNING:root:There are 3 tokens in class zurich\n",
      "WARNING:root:['limmat', 'flow', 'lake']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.0\n",
      "WARNING:root:Posterior: 0.08\n",
      "WARNING:root:Token flow appears once in class geneva\n",
      "WARNING:root:There are 4 tokens in class geneva\n",
      "WARNING:root:['rhône', 'flow', 'lake', 'geneva']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.25\n",
      "WARNING:root:Posterior: 0.15\n",
      "WARNING:root:Token lake appears once in class geneva\n",
      "WARNING:root:There are 4 tokens in class geneva\n",
      "WARNING:root:['rhône', 'flow', 'lake', 'geneva']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.25\n",
      "WARNING:root:Posterior: 0.15\n",
      "WARNING:root:Token geneva appears once in class geneva\n",
      "WARNING:root:There are 4 tokens in class geneva\n",
      "WARNING:root:['rhône', 'flow', 'lake', 'geneva']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.25\n",
      "WARNING:root:Posterior: 0.15\n",
      "WARNING:root:Token bear appears 0 times in class geneva\n",
      "WARNING:root:There are 4 tokens in class geneva\n",
      "WARNING:root:['rhône', 'flow', 'lake', 'geneva']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.0\n",
      "WARNING:root:Posterior: 0.08\n",
      "WARNING:root:Token rhône appears once in class geneva\n",
      "WARNING:root:There are 4 tokens in class geneva\n",
      "WARNING:root:['rhône', 'flow', 'lake', 'geneva']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.25\n",
      "WARNING:root:Posterior: 0.15\n",
      "WARNING:root:Token pit appears 0 times in class geneva\n",
      "WARNING:root:There are 4 tokens in class geneva\n",
      "WARNING:root:['rhône', 'flow', 'lake', 'geneva']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.0\n",
      "WARNING:root:Posterior: 0.08\n",
      "WARNING:root:Token river appears 0 times in class geneva\n",
      "WARNING:root:There are 4 tokens in class geneva\n",
      "WARNING:root:['rhône', 'flow', 'lake', 'geneva']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.0\n",
      "WARNING:root:Posterior: 0.08\n",
      "WARNING:root:Token limmat appears 0 times in class geneva\n",
      "WARNING:root:There are 4 tokens in class geneva\n",
      "WARNING:root:['rhône', 'flow', 'lake', 'geneva']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.0\n",
      "WARNING:root:Posterior: 0.08\n",
      "WARNING:root:Token near appears 0 times in class geneva\n",
      "WARNING:root:There are 4 tokens in class geneva\n",
      "WARNING:root:['rhône', 'flow', 'lake', 'geneva']\n",
      "WARNING:root:Vocab size: 9\n",
      "WARNING:root:Vanilla posterior: 0.0\n",
      "WARNING:root:Posterior: 0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17\n"
     ]
    }
   ],
   "source": [
    "# The method get_posterior_probabilities expects the training data in the form of a data frame\n",
    "training_data = pd.DataFrame(columns=['documents', 'labels'])\n",
    "training_data['documents']=normalized_training_corpus\n",
    "training_data['labels']=training_labels\n",
    "\n",
    "# BEGIN_REMOVE\n",
    "posterior=naive_bayes.get_posterior_probabilities(training_data, verbose=True)\n",
    "print (\"%.2f\"%posterior['limmat', 'zurich'])\n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code so we can train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class naive_bayes(naive_bayes):\n",
    "    \n",
    "    def train(self, training_data, timing=False):\n",
    "\n",
    "            import logging\n",
    "\n",
    "            classes = training_data['labels']\n",
    "\n",
    "            # BEGIN_REMOVE            \n",
    "            with ms_timer() as timer:\n",
    "\n",
    "                P_c = \\\n",
    "                [(training_data['labels']==tagged_class).sum()/len(training_data) \\\n",
    "                 for tagged_class in classes]\n",
    "            if timing:\n",
    "                logging.warning('Priors probabilities computed in '+\"%.2f\"%timer.get_elapsed_time()+\" ms\")\n",
    "            \n",
    "            with ms_timer() as timer:\n",
    "                \n",
    "                posterior_p=self.get_posterior_probabilities(training_data)\n",
    "            if timing:    \n",
    "                logging.warning('Posterior probabilities computed in '+\"%.2f\"%timer.get_elapsed_time()+\" ms\")    \n",
    "            # END_REMOVE\n",
    "            \n",
    "            return P_c, posterior_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get to train the classifier. \n",
    "\n",
    "Print out the prior probabilities and the posterior probabilities and answer the following questions:\n",
    "\n",
    "a) What is the lowest posterior probability that you observe and why?\n",
    "\n",
    "b) What is the highest posterior probability that you observe and why?\n",
    "\n",
    "c) Why are the prior probabilities all 1/3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Priors probabilities computed in 1.07 ms\n",
      "WARNING:root:Posterior probabilities computed in 1.51 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probabilities:\n",
      "['0.33', '0.33', '0.33']\n",
      "Posterior probabilities:\n",
      "      (token, class)  post_p\n",
      "0       (flow, bern)    0.07\n",
      "1       (lake, bern)    0.07\n",
      "2     (geneva, bern)    0.07\n",
      "3       (bear, bern)    0.21\n",
      "4      (rhône, bern)    0.07\n",
      "5        (pit, bern)    0.14\n",
      "6      (river, bern)    0.14\n",
      "7     (limmat, bern)    0.07\n",
      "8       (near, bern)    0.14\n",
      "9     (flow, zurich)    0.17\n",
      "10    (lake, zurich)    0.17\n",
      "11  (geneva, zurich)    0.08\n",
      "12    (bear, zurich)    0.08\n",
      "13   (rhône, zurich)    0.08\n",
      "14     (pit, zurich)    0.08\n",
      "15   (river, zurich)    0.08\n",
      "16  (limmat, zurich)    0.17\n",
      "17    (near, zurich)    0.08\n",
      "18    (flow, geneva)    0.15\n",
      "19    (lake, geneva)    0.15\n",
      "20  (geneva, geneva)    0.15\n",
      "21    (bear, geneva)    0.08\n",
      "22   (rhône, geneva)    0.15\n",
      "23     (pit, geneva)    0.08\n",
      "24   (river, geneva)    0.08\n",
      "25  (limmat, geneva)    0.08\n",
      "26    (near, geneva)    0.08\n"
     ]
    }
   ],
   "source": [
    "nb=naive_bayes()\n",
    "P_c, posterior_p=nb.train(training_data, timing=True) \n",
    "\n",
    "# BEGIN_REMOVE\n",
    "print ('Prior probabilities:')\n",
    "print ([str(round(x, 2)) for x in P_c])\n",
    "\n",
    "print ('Posterior probabilities:')\n",
    "df=pd.DataFrame()\n",
    "df['(token, class)']=[x for x in posterior_p.keys()]\n",
    "df['post_p']=list(map(lambda x:round(x, 2), posterior_p.values()))\n",
    "print (df.to_string())\n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get to do the classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class naive_bayes(naive_bayes):\n",
    "    \n",
    "    \n",
    "    def classify_document (self, training_data, test_document, logarithmic=True):\n",
    "\n",
    "            import logging\n",
    "            from functools import reduce\n",
    "            import math\n",
    "            from nltk.tokenize import word_tokenize\n",
    "\n",
    "            classes = list(set(training_data['labels']))\n",
    "            \n",
    "            \n",
    "            P_c, posterior_p=self.train(training_data)\n",
    "\n",
    "            NB=dict()\n",
    "            \n",
    "            normalized_test_document = normalize(test_document, keep_inflected=False)\n",
    "\n",
    "            for index, c in enumerate(classes):\n",
    "\n",
    "                posterior_logsum=0\n",
    "                \n",
    "                for token in normalized_test_document:\n",
    "                    \n",
    "                    \n",
    "                    try:\n",
    "                        posterior_logsum=posterior_logsum+math.log(posterior_p[token, c], 10)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                if posterior_logsum==0:\n",
    "                        logging.error ('Classification failure: insufficient info')\n",
    "            \n",
    "                NB[c]=round(posterior_logsum+math.log(P_c[index], 10), 2)\n",
    "                \n",
    "            return max(NB, key=NB.get), NB \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your classifier with the test document *The name of the city comes from the word 'bear'.* What goes wrong? Can you fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the city comes from the word 'bear'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Classification failure: insufficient info\n",
      "ERROR:root:Classification failure: insufficient info\n",
      "ERROR:root:Classification failure: insufficient info\n",
      "WARNING:root:Classification completed in 5.51 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bern', {'bern': -0.48, 'zurich': -0.48, 'geneva': -0.48})\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_REMOVE\n",
    "\n",
    "import logging\n",
    "nb=naive_bayes()\n",
    "test_corpus = \"The name of the city comes from the word 'bear'\"\n",
    "test_labels = \"bern\"\n",
    "\n",
    "print (test_corpus)\n",
    "\n",
    "with ms_timer() as timer:\n",
    "    result = nb.classify_document(training_data, test_corpus)\n",
    "    \n",
    "logging.warning('Classification completed in '+\"%.2f\"%timer.get_elapsed_time()+\" ms\")    \n",
    "print (result)    \n",
    "\n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Classification completed in 4.23 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the city comes from the word bear\n",
      "('bern', {'bern': -1.15, 'zurich': -1.56, 'geneva': -1.59})\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_REMOVE\n",
    "\n",
    "import logging\n",
    "nb=naive_bayes()\n",
    "test_corpus = \"The name of the city comes from the word 'bear'\"\n",
    "test_labels = \"bern\"\n",
    "\n",
    "test_corpus=test_corpus.replace('\\'', '')\n",
    "\n",
    "print (test_corpus)\n",
    "\n",
    "with ms_timer() as timer:\n",
    "    result = nb.classify_document(training_data, test_corpus)\n",
    "    \n",
    "logging.warning('Classification completed in '+\"%.2f\"%timer.get_elapsed_time()+\" ms\")    \n",
    "print (result)    \n",
    "\n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you explain the performance of your classifier on the following test corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Classification of \"We saw the bears there.\" completed in 4.38 ms\n",
      "WARNING:root:Classification of \"We crossed the Rhône.\" completed in 3.33 ms\n",
      "WARNING:root:Classification of \"There is no lake.\" completed in 3.37 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Classifying: We saw the bears there.\n",
      "('bern', {'bern': -1.15, 'zurich': -1.56, 'geneva': -1.59})\n",
      "correct label: bern\n",
      "\n",
      " Classifying: We crossed the Rhône.\n",
      "('geneva', {'bern': -1.62, 'zurich': -1.56, 'geneva': -1.29})\n",
      "correct label: geneva\n",
      "\n",
      " Classifying: There is no lake.\n",
      "('zurich', {'bern': -1.62, 'zurich': -1.26, 'geneva': -1.29})\n",
      "correct label: bern\n"
     ]
    }
   ],
   "source": [
    "test_corpus = ['We saw the bears there.', \n",
    "               'We crossed the Rhône.', \n",
    "               'There is no lake.',\n",
    "              ]\n",
    "test_labels = ['bern',\n",
    "               'geneva',\n",
    "               'bern',\n",
    "              ]\n",
    "\n",
    "nb=naive_bayes() \n",
    "\n",
    "\n",
    "\n",
    "for item in test_corpus:\n",
    "    print ('\\n Classifying: '+item)\n",
    "    with ms_timer() as timer:\n",
    "        result = nb.classify_document(training_data, item)\n",
    "    logging.warning('Classification of \\\"'+item+'\\\" completed in '+\"%.2f\"%timer.get_elapsed_time()+\" ms\")    \n",
    "    print (result)                                      \n",
    "    print ('correct label: '+test_labels[test_corpus.index(item)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your classifier with the one-sentence document \"The federal capital is pretty.\" What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Classification failure: insufficient info\n",
      "ERROR:root:Classification failure: insufficient info\n",
      "ERROR:root:Classification failure: insufficient info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bern', {'bern': -0.48, 'zurich': -0.48, 'geneva': -0.48})\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_REMOVE\n",
    "test_corpus = \"The federal capital is pretty.\"\n",
    "test_labels = \"bern\"\n",
    "\n",
    "# Your classifier fails because your test document contains a previously unseen word. \n",
    "print (nb.classify_document(training_data, test_corpus))\n",
    "# END_REMOVE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
