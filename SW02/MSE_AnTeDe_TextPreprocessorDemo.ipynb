{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logocompact/300x300/1613732714/logo-mse.png \"MSE Logo\") \n",
    "\n",
    "# AnTeDe : Demo of a Preprocessing Pipeline (related to Lab 2)\n",
    "\n",
    "by Fabian Märki (FHNW), revised by Andrei Popescu-Belis (HES-SO)\n",
    "\n",
    "## Summary\n",
    "The aim of this demo is to show a number of text preprocessing steps for Vector Models in the context of scikit learn's processing pipeline.  These steps can be compared with those seen in Lab 1 using NLTK.  There is no code to complete and no question to answer in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inflect in /opt/conda/lib/python3.8/site-packages (5.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "# Here are some of the packages that will be demonstrated.\n",
    "# Make sure they are all installed in your conda environment.\n",
    "# If not, use \"conda install PACKAGENAME\" and see notes below.\n",
    "!pip install inflect\n",
    "\n",
    "import os\n",
    "import string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contraction\n",
    "[Contraction](https://github.com/kootenpv/contractions) is a simple library solely for expanding English contractions.  \n",
    "\n",
    "_Tips for installing:_ `conda install contractions` may not work, so better try `pip install contractions`.  The package has a small number of dependencies, but one of them ([`pyahocorasick`](https://github.com/WojciechMula/pyahocorasick)) may trigger C compilation errors upon installation with `pip` and will not work with `conda install` but should work with `conda install -c conda-forge pyahocorasick` (see Anaconda [documentation](https://anaconda.org/conda-forge/pyahocorasick)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am tall, you are tall, she is tall but he is not tall. But he is an apple in his hand is not correct.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions.fix(\"I'm tall, you're tall, she's tall but he isn't tall. But he's an apple in his hand isn't correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inflect\n",
    "[Inflect](https://pypi.org/project/inflect/) is a library for manipulating Engslih word inflections.  It can generate plural or singular nouns, ordinals, and indefinite articles, and can converting numbers written in digits to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one thousand, two hundred and thirty-four'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = inflect.engine()\n",
    "p.number_to_words(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicodedata\n",
    "This [Python 3 module](https://docs.python.org/3/library/unicodedata.html) provides access to the Unicode Character Database which defines character properties for all Unicode characters.  It can help to cleanup text with character conversion flaws, but may or may not be a good idea for languages which rely heavily on special characters like French or German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o o o e e e o o a a a n n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFKD', \"o ö ô e é è o ö a ä à n ñ\").encode('ascii', 'ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup \n",
    "[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is a useful library for parsing and extracting data from HTML and XML documents.  Cleaning HTML can be a cumbersome task without such a pacckage, as discussed [here](https://matix.io/extract-text-from-webpage-using-beautifulsoup-and-python/) or [here](https://www.pluralsight.com/guides/extracting-data-html-beautifulsoup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTitle Goes Here\\nBolded Text\\nItalicized Text\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(\"\"\"\n",
    "<h1>Title Goes Here</h1> \n",
    "\n",
    "<b>Bolded Text</b>\n",
    "<i>Italicized Text</i>\n",
    "\"\"\", \"html.parser\")\n",
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization with NLTK\n",
    "The purpose of lemmatization is to reduce different inflected forms of a word to a normalized one called _lemma_.  For example, a lemmatizer should be able to determine that _gone_, _going_ and _went_ all have the same lemma _go_.  The output of lemmatization is a proper word, so lemmatisation by simple suffix stripping (as with some stemming algorithms) is not sufficient.\n",
    "\n",
    "The goal of lemmatization is somehow similar to stemming (demonstrated below), as it maps several words into one common root, but the stem is not necessarily and actual word, while the lemma is.\n",
    "\n",
    "You will need to download WordNet once in your conda environment, by executing `nltk.download('wordnet')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() # from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'striped',\n",
       " 'bat',\n",
       " 'are',\n",
       " 'hanging',\n",
       " 'on',\n",
       " 'their',\n",
       " 'foot',\n",
       " 'for',\n",
       " 'rest',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lemmatizer.lemmatize(w) for w in nltk.word_tokenize(\"The striped bats are hanging on their feet for rest.\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The success of lemmatization depends on indicating the correct part-of-speech to the lemmatizer (as the second argument, named `pos`).  Part-of-speech tagging will be discussed in a later course, but for now you can download (once) a tagger for NLTK, for instance by running `nltk.download('averaged_perceptron_tagger')`.\n",
    "\n",
    "This tagger will label every word with its part-of-speech, but the tags have to be converted into those known by WordNet, i.e. NOUN, ADJ, ADV, or VERB (see the [NLTK doc here](http://www.nltk.org/api/nltk.corpus.reader.html?highlight=nltk%20corpus%20wordnet#module-nltk.corpus.reader.wordnet), and a [SO question here](https://stackoverflow.com/questions/51634328/wordnetlemmatizer-different-handling-of-wn-adj-and-wn-adj-sat)).\n",
    "\n",
    "To convert the tags, we define the converter function `get_wordnet_pos`.  We then get the result of lemmatization, and can compare it with the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):    \n",
    "    if tag[0] == \"J\":\n",
    "        return wordnet.ADJ\n",
    "    elif tag[0] == \"N\":\n",
    "        return wordnet.NOUN\n",
    "    elif tag[0] == \"V\":\n",
    "        return wordnet.VERB\n",
    "    elif tag[0] == \"R\":\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'striped',\n",
       " 'bat',\n",
       " 'be',\n",
       " 'hang',\n",
       " 'on',\n",
       " 'their',\n",
       " 'foot',\n",
       " 'for',\n",
       " 'rest',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lemmatizer.lemmatize(w[0], get_wordnet_pos(w[1])) \n",
    " for w in nltk.pos_tag(nltk.word_tokenize(\"The striped bats were hanging on their feet for rest.\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming with NLTK\n",
    "Stemming is the process of reducing a word into its stem, i.e. its _root form_, which is not necessarily a word by itself. For example, the words _fish_, _fishes_ and _fishing_ all stem into _fish_, which is a correct word. On the other side, the words _study_, _studies_ and _studying_ stem into _studi_, which is not an English word.\n",
    "\n",
    "Commonly, stemming algorithms (a.k.a. stemmers) are based on rules for suffix stripping.  The most famous example is the **Porter stemmer**, introduced in the 1980's and currently implemented in a variety of programming languages.  The **Snowball stemmer** is an improved version of the Porter stemmer.\n",
    "\n",
    "Traditionally, search engines and other IR applications have applied stemming to improve the chance of matching different forms of a word, treating them as interchangeable, which may or may not be appropriate when searching.\n",
    "\n",
    "Stemming can be seen as a quick and dirty method of chopping off words to their root forms, working especially on English.  Lemmatization is operation that requires more linguistic knowledge, sich as dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "troubl troubl troubl\n",
      "happi happier happiest\n",
      "cat cat\n",
      "is are be\n"
     ]
    }
   ],
   "source": [
    "ls = SnowballStemmer(\"english\") # from NLTK\n",
    "print(ls.stem(\"trouble\"), ls.stem(\"troubling\"), ls.stem(\"troubled\"))\n",
    "print(ls.stem(\"happy\"), ls.stem(\"happier\"), ls.stem(\"happiest\"))\n",
    "print(ls.stem(\"cat\"), ls.stem(\"cats\"))\n",
    "print(ls.stem(\"is\"), ls.stem(\"are\"), ls.stem(\"be\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing with the Gensim library\n",
    "[Gensim](https://radimrehurek.com/gensim/) is a Python library widely used for topic modeling.  It provides handy utilities to preprocess text, documented [here](https://radimrehurek.com/gensim/parsing/preprocessing.html) and [here](https://github.com/thunlp/topical_word_embeddings/blob/master/TWE-2/gensim/parsing/preprocessing.py).  A simple example is as follows (don't forget to `import gensim`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello',\n",
       "  'world',\n",
       "  'weather',\n",
       "  'todai',\n",
       "  'isn',\n",
       "  'tall',\n",
       "  'tall',\n",
       "  'isn',\n",
       "  'tall',\n",
       "  'appl',\n",
       "  'hand',\n",
       "  'isn',\n",
       "  'correct',\n",
       "  'titl',\n",
       "  'goe',\n",
       "  'bold',\n",
       "  'text',\n",
       "  'italic',\n",
       "  'text',\n",
       "  'stripe',\n",
       "  'bat',\n",
       "  'hang']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_documents([\"\"\"\n",
    "<i>Hello</i> <b>World</b> 9!\", \"Th3     weather_is really g00d today, isn't it?\n",
    "I'm tall, you're tall, but he isn't tall. But he's an apple in his hand isn't correct.\n",
    "o ö ô e é è o ö a ä à n ñ\n",
    "<h1>Title Goes Here</h1> \n",
    "\n",
    "<b>Bolded Text</b>\n",
    "<i>Italicized Text</i>\n",
    "The striped bats are hanging.\n",
    "\"\"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing with our own TextPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's program our own TextPreprocessor class, compatible with the processing pipeline of the `scikit-learn` library.  \n",
    "\n",
    "This class is available in the file `TextPreprocessor.py` provided with the lab, which should be imported.  You can use it in later AnTeDe labs.  It is inspired by two documents available [here](https://towardsdatascience.com/text-preprocessing-steps-and-universal-pipeline-94233cb6725a) and [here](https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextPreprocessor import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use here a shortened version of the Lee Background Corpus [described here](http://www.socsci.uci.edu/~mdlee/lee_pincombe_welsh_document.PDF). The shortened version consists of 300 documents selected from the Australian Broadcasting Corporation's news mail service. It consists of texts of headline stories from around the years 2000-2001.  It is available as test data in the `gensim` package, so you do not need to download it separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents into a Pandas data frame.  Code inspired from:\n",
    "# https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis_tutorial/topic_modelling.ipynb\n",
    "\n",
    "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
    "lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n",
    "text = open(lee_train_file).read().splitlines()\n",
    "data_df = pd.DataFrame({'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 3.3695677240002624\n"
     ]
    }
   ],
   "source": [
    "# Run our TextPreprocessor and chronometer it.\n",
    "\n",
    "language = 'english'\n",
    "stop_words = set(stopwords.words(language)) # from NLTK: do nltk.download('stopwords') once.\n",
    "for sw in ['\\\"', '\\'', '\\'\\'', '`', '``', '\\'s']:\n",
    "    stop_words.add(sw)\n",
    "\n",
    "processor = TextPreprocessor(\n",
    "    language = language,\n",
    "    pos_tags = {wordnet.ADJ, wordnet.NOUN},\n",
    "    stopwords = stop_words,\n",
    ")\n",
    "start = timer()\n",
    "data_df['processed'] = processor.transform(data_df['text'])\n",
    "end = timer()\n",
    "print(\"Took: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hundreds of people have been forced to vacate their homes in the Southern Highlands of New South Wales as strong winds today pushed a huge bushfire towards the town of Hill Top. A new blaze near Goulburn, south-west of Sydney, has forced the closure of the Hume Highway. At about 4:00pm AEDT, a marked deterioration in the weather as a storm cell moved east across the Blue Mountains forced authorities to make a decision to evacuate people from homes in outlying streets at Hill Top in the New South Wales southern highlands. An estimated 500 residents have left their homes for nearby Mittagong. The New South Wales Rural Fire Service says the weather conditions which caused the fire to burn in a finger formation have now eased and about 60 fire units in and around Hill Top are optimistic of defending all properties. As more than 100 blazes burn on New Year's Eve in New South Wales, fire crews have been called to new fire at Gunning, south of Goulburn. While few details are available at this stage, fire authorities says it has closed the Hume Highway in both directions. Meanwhile, a new fire in Sydney's west is no longer threatening properties in the Cranebrook area. Rain has fallen in some parts of the Illawarra, Sydney, the Hunter Valley and the north coast. But the Bureau of Meteorology's Claire Richards says the rain has done little to ease any of the hundred fires still burning across the state. \"The falls have been quite isolated in those areas and generally the falls have been less than about five millimetres,\" she said. \"In some places really not significant at all, less than a millimetre, so there hasn't been much relief as far as rain is concerned. \"In fact, they've probably hampered the efforts of the firefighters more because of the wind gusts that are associated with those thunderstorms.\"  \n",
      "\n",
      "hundred people vacate home southern highland new south wale strong wind today huge bushfire towards town hill top new blaze near goulburn south-west sydney closure hume highway 4:00pm aedt marked deterioration weather storm cell east across blue mountain authority decision evacuate people home street hill top new south wale southern highland resident left home mittagong new south wale rural fire service weather condition fire burn finger formation fire unit around hill top optimistic property blaze burn new year eve new south wale fire crew new fire south goulburn detail available stage fire authority hume highway direction new fire sydney west longer property cranebrook area rain part illawarra sydney hunter valley north coast bureau meteorology claire richards rain little ease fire burning across state fall isolated area fall five millimetre place significant millimetre much relief rain concerned fact effort firefighter wind gust thunderstorm\n"
     ]
    }
   ],
   "source": [
    "print(data_df['text'].iloc[0], '\\n')  # pandas.DataFrame.iloc - integer location for selection by position\n",
    "print(data_df['processed'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use gensim's preprocessing utility function introduced above.  This does not perform lemmatization, but stemming (on English), and generates a list of words.  We can compare the timing and then the outputs on the first text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.21547094000015932\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "data_df['processed_gensim'] = preprocess_documents(data_df['text'])\n",
    "end = timer()\n",
    "print(\"Took: \"+str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hundr', 'peopl', 'forc', 'vacat', 'home', 'southern', 'highland', 'new', 'south', 'wale', 'strong', 'wind', 'todai', 'push', 'huge', 'bushfir', 'town', 'hill', 'new', 'blaze', 'near', 'goulburn', 'south', 'west', 'sydnei', 'forc', 'closur', 'hume', 'highwai', 'aedt', 'mark', 'deterior', 'weather', 'storm', 'cell', 'move', 'east', 'blue', 'mountain', 'forc', 'author', 'decis', 'evacu', 'peopl', 'home', 'outli', 'street', 'hill', 'new', 'south', 'wale', 'southern', 'highland', 'estim', 'resid', 'left', 'home', 'nearbi', 'mittagong', 'new', 'south', 'wale', 'rural', 'servic', 'sai', 'weather', 'condit', 'caus', 'burn', 'finger', 'format', 'eas', 'unit', 'hill', 'optimist', 'defend', 'properti', 'blaze', 'burn', 'new', 'year', 'ev', 'new', 'south', 'wale', 'crew', 'call', 'new', 'gun', 'south', 'goulburn', 'detail', 'avail', 'stage', 'author', 'sai', 'close', 'hume', 'highwai', 'direct', 'new', 'sydnei', 'west', 'longer', 'threaten', 'properti', 'cranebrook', 'area', 'rain', 'fallen', 'part', 'illawarra', 'sydnei', 'hunter', 'vallei', 'north', 'coast', 'bureau', 'meteorolog', 'clair', 'richard', 'sai', 'rain', 'littl', 'eas', 'fire', 'burn', 'state', 'fall', 'isol', 'area', 'gener', 'fall', 'millimetr', 'said', 'place', 'signific', 'millimetr', 'hasn', 'relief', 'far', 'rain', 'concern', 'fact', 'probabl', 'hamper', 'effort', 'firefight', 'wind', 'gust', 'associ', 'thunderstorm']\n"
     ]
    }
   ],
   "source": [
    "print(data_df['processed_gensim'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now aware of three ways of pre-processing texts, for instance to use them as a document database for information retrieval:\n",
    "* a set of NLTK functions;\n",
    "* the in-house class `TextPreprocessing`;\n",
    "* gensim's `preprocess_documents` function.\n",
    "Please proceed now to Lab 2, where you will set up a document retrieval system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
