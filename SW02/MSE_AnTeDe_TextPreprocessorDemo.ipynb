{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logocompact/300x300/1613732714/logo-mse.png \"MSE Logo\") \n",
    "\n",
    "# AnTeDe : Demo of a Preprocessing Pipeline (related to Lab 2)\n",
    "\n",
    "by Fabian Märki (FHNW), revised by Andrei Popescu-Belis (HES-SO)\n",
    "\n",
    "## Summary\n",
    "The aim of this demo is to show a number of text preprocessing steps for Vector Models in the context of scikit learn's processing pipeline.  These steps can be compared with those seen in Lab 1 using NLTK.  There is no code to complete and no question to answer in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some of the packages that will be demonstrated.\n",
    "# Make sure they are all installed in your conda environment.\n",
    "# If not, use \"conda install PACKAGENAME\" and see notes below.\n",
    "\n",
    "import os\n",
    "import string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contraction\n",
    "[Contraction](https://github.com/kootenpv/contractions) is a simple library solely for expanding English contractions.  \n",
    "\n",
    "_Tips for installing:_ `conda install contractions` may not work, so better try `pip install contractions`.  The package has a small number of dependencies, but one of them ([`pyahocorasick`](https://github.com/WojciechMula/pyahocorasick)) may trigger C compilation errors upon installation with `pip` and will not work with `conda install` but should work with `conda install -c conda-forge pyahocorasick` (see Anaconda [documentation](https://anaconda.org/conda-forge/pyahocorasick)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions.fix(\"I'm tall, you're tall, she's tall but he isn't tall. But he's an apple in his hand isn't correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inflect\n",
    "[Inflect](https://pypi.org/project/inflect/) is a library for manipulating Engslih word inflections.  It can generate plural or singular nouns, ordinals, and indefinite articles, and can converting numbers written in digits to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = inflect.engine()\n",
    "p.number_to_words(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicodedata\n",
    "This [Python 3 module](https://docs.python.org/3/library/unicodedata.html) provides access to the Unicode Character Database which defines character properties for all Unicode characters.  It can help to cleanup text with character conversion flaws, but may or may not be a good idea for languages which rely heavily on special characters like French or German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicodedata.normalize('NFKD', \"o ö ô e é è o ö a ä à n ñ\").encode('ascii', 'ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup \n",
    "[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is a useful library for parsing and extracting data from HTML and XML documents.  Cleaning HTML can be a cumbersome task without such a pacckage, as discussed [here](https://matix.io/extract-text-from-webpage-using-beautifulsoup-and-python/) or [here](https://www.pluralsight.com/guides/extracting-data-html-beautifulsoup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(\"\"\"\n",
    "<h1>Title Goes Here</h1> \n",
    "\n",
    "<b>Bolded Text</b>\n",
    "<i>Italicized Text</i>\n",
    "\"\"\", \"html.parser\")\n",
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization with NLTK\n",
    "The purpose of lemmatization is to reduce different inflected forms of a word to a normalized one called _lemma_.  For example, a lemmatizer should be able to determine that _gone_, _going_ and _went_ all have the same lemma _go_.  The output of lemmatization is a proper word, so lemmatisation by simple suffix stripping (as with some stemming algorithms) is not sufficient.\n",
    "\n",
    "The goal of lemmatization is somehow similar to stemming (demonstrated below), as it maps several words into one common root, but the stem is not necessarily and actual word, while the lemma is.\n",
    "\n",
    "You will need to download WordNet once in your conda environment, by executing `nltk.download('wordnet')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() # from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lemmatizer.lemmatize(w) for w in nltk.word_tokenize(\"The striped bats are hanging on their feet for rest.\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The success of lemmatization depends on indicating the correct part-of-speech to the lemmatizer (as the second argument, named `pos`).  Part-of-speech tagging will be discussed in a later course, but for now you can download (once) a tagger for NLTK, for instance by running `nltk.download('averaged_perceptron_tagger')`.\n",
    "\n",
    "This tagger will label every word with its part-of-speech, but the tags have to be converted into those known by WordNet, i.e. NOUN, ADJ, ADV, or VERB (see the [NLTK doc here](http://www.nltk.org/api/nltk.corpus.reader.html?highlight=nltk%20corpus%20wordnet#module-nltk.corpus.reader.wordnet), and a [SO question here](https://stackoverflow.com/questions/51634328/wordnetlemmatizer-different-handling-of-wn-adj-and-wn-adj-sat)).\n",
    "\n",
    "To convert the tags, we define the converter function `get_wordnet_pos`.  We then get the result of lemmatization, and can compare it with the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):    \n",
    "    if tag[0] == \"J\":\n",
    "        return wordnet.ADJ\n",
    "    elif tag[0] == \"N\":\n",
    "        return wordnet.NOUN\n",
    "    elif tag[0] == \"V\":\n",
    "        return wordnet.VERB\n",
    "    elif tag[0] == \"R\":\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lemmatizer.lemmatize(w[0], get_wordnet_pos(w[1])) \n",
    " for w in nltk.pos_tag(nltk.word_tokenize(\"The striped bats were hanging on their feet for rest.\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming with NLTK\n",
    "Stemming is the process of reducing a word into its stem, i.e. its _root form_, which is not necessarily a word by itself. For example, the words _fish_, _fishes_ and _fishing_ all stem into _fish_, which is a correct word. On the other side, the words _study_, _studies_ and _studying_ stem into _studi_, which is not an English word.\n",
    "\n",
    "Commonly, stemming algorithms (a.k.a. stemmers) are based on rules for suffix stripping.  The most famous example is the **Porter stemmer**, introduced in the 1980's and currently implemented in a variety of programming languages.  The **Snowball stemmer** is an improved version of the Porter stemmer.\n",
    "\n",
    "Traditionally, search engines and other IR applications have applied stemming to improve the chance of matching different forms of a word, treating them as interchangeable, which may or may not be appropriate when searching.\n",
    "\n",
    "Stemming can be seen as a quick and dirty method of chopping off words to their root forms, working especially on English.  Lemmatization is operation that requires more linguistic knowledge, sich as dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = SnowballStemmer(\"english\") # from NLTK\n",
    "print(ls.stem(\"trouble\"), ls.stem(\"troubling\"), ls.stem(\"troubled\"))\n",
    "print(ls.stem(\"happy\"), ls.stem(\"happier\"), ls.stem(\"happiest\"))\n",
    "print(ls.stem(\"cat\"), ls.stem(\"cats\"))\n",
    "print(ls.stem(\"is\"), ls.stem(\"are\"), ls.stem(\"be\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing with the Gensim library\n",
    "[Gensim](https://radimrehurek.com/gensim/) is a Python library widely used for topic modeling.  It provides handy utilities to preprocess text, documented [here](https://radimrehurek.com/gensim/parsing/preprocessing.html) and [here](https://github.com/thunlp/topical_word_embeddings/blob/master/TWE-2/gensim/parsing/preprocessing.py).  A simple example is as follows (don't forget to `import gensim`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_documents([\"\"\"\n",
    "<i>Hello</i> <b>World</b> 9!\", \"Th3     weather_is really g00d today, isn't it?\n",
    "I'm tall, you're tall, but he isn't tall. But he's an apple in his hand isn't correct.\n",
    "o ö ô e é è o ö a ä à n ñ\n",
    "<h1>Title Goes Here</h1> \n",
    "\n",
    "<b>Bolded Text</b>\n",
    "<i>Italicized Text</i>\n",
    "The striped bats are hanging.\n",
    "\"\"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing with our own TextPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's program our own TextPreprocessor class, compatible with the processing pipeline of the `scikit-learn` library.  \n",
    "\n",
    "This class is available in the file `TextPreprocessor.py` provided with the lab, which should be imported.  You can use it in later AnTeDe labs.  It is inspired by two documents available [here](https://towardsdatascience.com/text-preprocessing-steps-and-universal-pipeline-94233cb6725a) and [here](https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextPreprocessor import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use here a shortened version of the Lee Background Corpus [described here](http://www.socsci.uci.edu/~mdlee/lee_pincombe_welsh_document.PDF). The shortened version consists of 300 documents selected from the Australian Broadcasting Corporation's news mail service. It consists of texts of headline stories from around the years 2000-2001.  It is available as test data in the `gensim` package, so you do not need to download it separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents into a Pandas data frame.  Code inspired from:\n",
    "# https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis_tutorial/topic_modelling.ipynb\n",
    "\n",
    "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
    "lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n",
    "text = open(lee_train_file).read().splitlines()\n",
    "data_df = pd.DataFrame({'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our TextPreprocessor and chronometer it.\n",
    "\n",
    "language = 'english'\n",
    "stop_words = set(stopwords.words(language)) # from NLTK: do nltk.download('stopwords') once.\n",
    "for sw in ['\\\"', '\\'', '\\'\\'', '`', '``', '\\'s']:\n",
    "    stop_words.add(sw)\n",
    "\n",
    "processor = TextPreprocessor(\n",
    "    language = language,\n",
    "    pos_tags = {wordnet.ADJ, wordnet.NOUN},\n",
    "    stopwords = stop_words,\n",
    ")\n",
    "start = timer()\n",
    "data_df['processed'] = processor.transform(data_df['text'])\n",
    "end = timer()\n",
    "print(\"Took: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_df['text'].iloc[0], '\\n')  # pandas.DataFrame.iloc - integer location for selection by position\n",
    "print(data_df['processed'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use gensim's preprocessing utility function introduced above.  This does not perform lemmatization, but stemming (on English), and generates a list of words.  We can compare the timing and then the outputs on the first text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "data_df['processed_gensim'] = preprocess_documents(data_df['text'])\n",
    "end = timer()\n",
    "print(\"Took: \"+str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_df['processed_gensim'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now aware of three ways of pre-processing texts, for instance to use them as a document database for information retrieval:\n",
    "* a set of NLTK functions;\n",
    "* the in-house class `TextPreprocessing`;\n",
    "* gensim's `preprocess_documents` function.\n",
    "Please proceed now to Lab 2, where you will set up a document retrieval system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cours",
   "language": "python",
   "name": "cours"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
