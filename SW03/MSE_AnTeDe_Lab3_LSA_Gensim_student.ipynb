{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logocompact/300x300/1613732714/logo-mse.png \"MSE Logo\") \n",
    "\n",
    "# AnTeDe Lab3: Latent Semantic Analysis with Gensim\n",
    "\n",
    "by Andrei Popescu-Belis (HES-SO), based on material by Fabian MÃ¤rki (FHNW) and Heiho Hahn (FHSG)\n",
    "\n",
    "## Summary\n",
    "The aim of this lab is to perform LSA on a small corpus of news.  You will use the LSA word vectors to estimate word similarity, and then to perform ranked retrieval given a query. \n",
    "\n",
    "<font color='green'>Please answer the questions in green within this notebook, and submit the completed notebook under the corresponding homework on Moodle.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.8/site-packages (3.8.3)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.8/site-packages (from gensim) (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.19.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.5.4)\n",
      "Requirement already satisfied: contractions in /opt/conda/lib/python3.8/site-packages (0.0.48)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/conda/lib/python3.8/site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in /opt/conda/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (1.4.1)\n",
      "Requirement already satisfied: anyascii in /opt/conda/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (0.1.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os    \n",
    "import nltk\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from TextPreprocessor import *\n",
    "from gensim import models, corpora, similarities\n",
    "from gensim.models import LsiModel, LdaModel, LdaMulticore\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used in this lab the same set of 300 Australian that you used in Lab 2.  It is a shortened version of the Lee Background Corpus [described here](http://www.socsci.uci.edu/~mdlee/lee_pincombe_welsh_document.PDF) and it is available with the **gensim** package that you installed.  The following code will load the documents into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hundreds of people have been forced to vacate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Indian security forces have shot dead eight su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The national road toll for the Christmas-New Y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Hundreds of people have been forced to vacate ...\n",
       "1  Indian security forces have shot dead eight su...\n",
       "2  The national road toll for the Christmas-New Y..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code inspired from:\n",
    "# https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis_tutorial/topic_modelling.ipynb\n",
    "\n",
    "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
    "lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n",
    "text = open(lee_train_file).read().splitlines()\n",
    "data_df = pd.DataFrame({'text': text})\n",
    "data_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "You will need first to preprocess the data through the following stages:\n",
    "1. tokenization\n",
    "2. stopword removal\n",
    "2. POS-based filtering (optional)\n",
    "3. lemmatization or stemming (optional)\n",
    "4. addition of bigrams to each document (optional)\n",
    "5. filtering of infrequent words\n",
    "6. inspection and filtering of frequent words\n",
    "\n",
    "You can use NLTK as in Lab 1, or our in-house `TextPreprocessor.py` file as in Lab 2.\n",
    "\n",
    "<font color='green'>Please state here which solution you use and list stages you implement.</font>\n",
    "I'll use the `TextPreprocessor.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write here the preprocessing instructions if you use TextPreprocessor.py\n",
    "language = 'english'\n",
    "stop_words = set(stopwords.words(language))\n",
    "# Extend the list here:\n",
    "for sw in ['\\\"', '\\'', '\\'\\'', '`', '``', '\\'s']:\n",
    "    stop_words.add(sw)\n",
    "\n",
    "\n",
    "processor = TextPreprocessor(\n",
    "# Add options here:\n",
    "    language = language,\n",
    "    pos_tags = {wordnet.ADJ, wordnet.NOUN},\n",
    "    stopwords = stop_words,\n",
    "    lemmantize = True,\n",
    "    stem = True,\n",
    "    remove_numbers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "union qanta mainten worker industri action compani reject offer disput parti privat talk yesterday industri relat commiss 3,000 mainten worker reject qanta wage freez nation secretari australian manufactur worker union amwu doug cameron union everyth possibl resolv disput qanta prepar accept privat arbitr altern worker industri action escal industri action necessari fair compani crush underfoot\n"
     ]
    }
   ],
   "source": [
    "data_df['processed'] = processor.transform(data_df['text'])\n",
    "print(data_df['processed'].iloc[120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['tokenized'] = data_df['processed'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write here the preprocessing instructions if you use NLTK\n",
    "# Not used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['union', 'qanta', 'mainten', 'worker', 'industri', 'action', 'compani', 'reject', 'offer', 'disput', 'parti', 'privat', 'talk', 'yesterday', 'industri', 'relat', 'commiss', '3,000', 'mainten', 'worker', 'reject', 'qanta', 'wage', 'freez', 'nation', 'secretari', 'australian', 'manufactur', 'worker', 'union', 'amwu', 'doug', 'cameron', 'union', 'everyth', 'possibl', 'resolv', 'disput', 'qanta', 'prepar', 'accept', 'privat', 'arbitr', 'altern', 'worker', 'industri', 'action', 'escal', 'industri', 'action', 'necessari', 'fair', 'compani', 'crush', 'underfoot']\n"
     ]
    }
   ],
   "source": [
    "print(data_df['tokenized'].iloc[120])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make a list of all words from all articles.  Then, using `nltk.FreqDist`, consider the most frequent and the least frequent words.  If you find uninformative words among the most frequent ones, please remove them from the articles.  Similarly, please remove from articles the words appearing fewer than 2 or 3 times in the corpus.  <font color='green'> Please justify these choices. What is now the size of your vocabulary?</font> \n",
    "\n",
    "The freqency distribution shows that there are still numbers in the corpus, altough they should have been removed by the text preprocessor. Thus they will be removed. Before the removement of the infrequent words the vocabulary was $\\approx$ 4000 and after $\\approx$ 1500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mr', 306),\n",
       " ('australian', 178),\n",
       " ('new', 171),\n",
       " ('palestinian', 168),\n",
       " ('australia', 157),\n",
       " ('peopl', 153),\n",
       " ('govern', 150),\n",
       " ('two', 136),\n",
       " ('u', 136),\n",
       " ('day', 131)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please write here all the necessary instructions.  You may use several cells.\n",
    "\n",
    "# remove non-alphanumeric words\n",
    "words = [w for ws in data_df['tokenized'] for w in ws if w.isalpha()]\n",
    "freq_dist = nltk.FreqDist(words)\n",
    "# most common\n",
    "freq_dist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cedric', 1),\n",
       " ('piolin', 1),\n",
       " ('fabric', 1),\n",
       " ('santoro', 1),\n",
       " ('apprais', 1),\n",
       " ('pro', 1),\n",
       " ('con', 1),\n",
       " ('overcam', 1),\n",
       " ('sebastien', 1),\n",
       " ('grosjean', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# least common\n",
    "freq_dist.most_common()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prudenti', 4),\n",
       " ('requir', 4),\n",
       " ('goodin', 4),\n",
       " ('hornsbi', 4),\n",
       " ('hindu', 4),\n",
       " ('saxeten', 4),\n",
       " ('liverpool', 4),\n",
       " ('arthur', 4),\n",
       " ('rubber', 4),\n",
       " ('cow', 4)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove words that have an occurence of < 3\n",
    "filtered = dict((word, freq) for word, freq in freq_dist.items() if freq > 3)\n",
    "freq_dist_filtered = nltk.FreqDist(filtered)\n",
    "freq_dist_filtered.most_common()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: 4278\n",
      "After filtering: 1521\n"
     ]
    }
   ],
   "source": [
    "# check how big the voci is\n",
    "voci = set(word for word, _ in freq_dist.items())\n",
    "voci_filtered = set(word for word, _ in freq_dist_filtered.items())\n",
    "print('Before filtering: %s' % len(voci))\n",
    "print('After filtering: %s' % len(voci_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'morn', 'restor', 'power', 'suppli', 'ten', 'thousand', 'home', 'storm', 'struck', 'queensland', 'last', 'night', 'forc', 'wind', 'tree', 'brought', 'power', 'line', 'home', 'car', 'energi', 'everi', 'avail', 'person', 'night', 'restor', 'power', 'locat', 'around', 'brisban', 'west', 'toowoomba', 'north', 'coast', 'brisban', 'protect', 'home', 'sever', 'storm', 'christma', 'four', 'peopl', 'high', 'power', 'line', 'across', 'car', 'insid', 'fierc', 'wind', 'sent', 'larg', 'tree', 'hous', 'one', 'injur']\n"
     ]
    }
   ],
   "source": [
    "# filter the data\n",
    "data_df['filtered'] = [[w for w in ws if w in voci_filtered] for ws in data_df['tokenized']]\n",
    "print(data_df['filtered'].iloc[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will write the Gensim commands to compute a term-document matrix from the above documents, then transform it using SVD, and truncate the result.  To learn what the commands are, please follow the [Topics and Tranformations tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html) from Gensim. \n",
    "\n",
    "<font color=\"green\">Please gather these commands into a function called `train_lsa`.  They should cover: dictionary creation, corpus mapping, computation of TF-IDF values, and creation of the LSA model.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lsa(filtered_texts, num_topics = 10):\n",
    "    # dictionary creation\n",
    "    dictionary = corpora.Dictionary(filtered_texts)\n",
    "    # corpus mapping, bow\n",
    "    corpus = [dictionary.doc2bow(text) for text in filtered_texts]\n",
    "    # TF-IDF model\n",
    "    tfidf = models.TfidfModel(corpus, normalize=True)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    # LSA model\n",
    "    lsa = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics)\n",
    "\n",
    "    return lsa, dictionary, corpus, corpus_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Please fix a `number_of_topics`, on the lower side of the range mentioned in the course.  Then, execute the cell that performs `train_lsa`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_model, dictionary, corpus, corpus_tfidf = train_lsa(data_df['filtered'], number_of_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Please display several topics found by LSA using the Gensim `print_topics` function.  Please explain in your own words the meaning of what is displayed.  How do you relate it with what was explained in the course on LSA?</font>\n",
    "\n",
    "The topics diplay the latent dimensions of the LSA transformation. The words 'palestina', 'israel' and 'arafat' are related and contribute the most for the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.336*\"palestinian\" + 0.222*\"isra\" + 0.197*\"arafat\" + 0.126*\"mr\" + 0.125*\"israel\" + 0.120*\"hama\" + 0.111*\"attack\" + 0.107*\"afghanistan\" + 0.106*\"forc\" + 0.103*\"gaza\"'),\n",
       " (1,\n",
       "  '-0.430*\"palestinian\" + -0.283*\"isra\" + -0.253*\"arafat\" + -0.157*\"israel\" + -0.155*\"hama\" + -0.135*\"gaza\" + 0.133*\"afghanistan\" + 0.108*\"bin\" + 0.107*\"laden\" + -0.106*\"sharon\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_model.print_topics(number_of_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Please define a function that returns the cosine similarity between two words (testing first if they are in the vocabulary). Please exemplify its value on two different word pairs, one of which should be obviously more similar than the other, and comment the values.</font>  You can get inspiration from this [Gensim Tutorial on Document Similarity](https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html).\n",
    "\n",
    "One can clearly see that the first similarity is higher in the two dimensions than the second one. The dimension of LSA was reduced to 2-D to better examine the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordsim(word1, word2, model, dictionary):\n",
    "    # get bow from words\n",
    "    w1_bow = dictionary.doc2bow([word1])\n",
    "    w2_bow = dictionary.doc2bow([word2])\n",
    "    \n",
    "    if len(w1_bow) == 0 or len(w2_bow) == 0:\n",
    "        raise Exception('Words not in dictionary!')\n",
    "    \n",
    "    # convert to LSA space\n",
    "    w1_lsa = model[w1_bow]\n",
    "    w2_lsa = model[w2_bow]\n",
    "    \n",
    "    # compute the similarity\n",
    "    sim = cosine_similarity(w1_lsa, w2_lsa)\n",
    "    \n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -0.15555533]\n",
      " [-0.39505025  0.96892901]]\n",
      "[[ 1.         -0.13398335]\n",
      " [ 0.00271741  0.99061584]]\n"
     ]
    }
   ],
   "source": [
    "# print here the cosine similiarities of several pairs and comment the results.\n",
    "sim_high = wordsim('palestinian', 'israel', lsa_model, dictionary)\n",
    "sim_low = wordsim('cow', 'gaza', lsa_model, dictionary)\n",
    "\n",
    "print(sim_high)\n",
    "print(sim_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Please use the [Gensim Tutorial on Document Similarity](https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html) to write a function that prints a list of words sorted by decreasing LSA similarity with a given word (giving the score too).  You won't have to use the cosine_similarity function here.  Please choose a \"query\" word and ten other words, apply your function, and comment the results.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ranking(word0, word_list, model, dictionary):\n",
    "    word0_bow = dictionary.doc2bow([word0])\n",
    "    words_bow = [dictionary.doc2bow([w]) for w in word_list]\n",
    "    \n",
    "    word0_lsa = model[word0_bow]\n",
    "    words_lsa = model[words_bow]\n",
    "    \n",
    "    index = similarities.MatrixSimilarity(words_lsa)\n",
    "    \n",
    "    # perform a similarity query against the corpus\n",
    "    sims = index[word0_lsa]  \n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "\n",
    "    for i, (doc_position, doc_score) in enumerate(sims):\n",
    "        print('{0}: \"{1}\"\", score: {2:.5f}'.format(i, word_list[doc_position], doc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \"palestinian\"\", score: 0.99993\n",
      "1: \"israel\"\", score: 0.99983\n",
      "2: \"cow\"\", score: -0.15248\n",
      "3: \"water\"\", score: -0.25935\n"
     ]
    }
   ],
   "source": [
    "# call here the function on your choice of words\n",
    "word_ranking('gaza', ['cow', 'water', 'palestinian', 'israel'], lsa_model, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write here your comments on the rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be clearly seen that the word has a higher score with the words \"palestina\" and \"israel\", since they often appear together. And it has a lower score with the words \"cow\" and \"water\" since they do not often appear together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Please select now a significantly larger number of topics, and train a new LSA model.  Perform the same `word_ranking` task as above and compare the new ranking with the previous one.  Which one seems better?</font>\n",
    "\n",
    "The first one with the fewer dimensions (2-D) performs much better, because it can find the similarities between the correct word pairs. The one with higher dimensions (300-D) is not able to find these similiarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \"palestinian\"\", score: 0.15100\n",
      "1: \"cow\"\", score: 0.01348\n",
      "2: \"water\"\", score: -0.01740\n",
      "3: \"israel\"\", score: -0.07435\n"
     ]
    }
   ],
   "source": [
    "lsa_model, dictionary, corpus, corpus_tfidf = train_lsa(data_df['filtered'], num_topics=300)\n",
    "word_ranking('gaza', ['cow', 'water', 'palestinian', 'israel'], lsa_model, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA with Gensim\n",
    "For a simple tutorial on using LDA with Gensim, please see https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for your work!  Please submit the notebook on Moodle before the next course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
