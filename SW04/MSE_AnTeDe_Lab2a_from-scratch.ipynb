{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnTeDe Lab 2: Text Classification - Part A\n",
    "\n",
    "## Session goal\n",
    "The goal of this session is to implement a Multinomial Naive Bayes classifier from scratch.\n",
    "\n",
    "## Data collection\n",
    "We are going to use a small toy dataset. Each document is a single sentence. The training data contains three documents, each from a different class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# these 3 lines are here for compatibility purposes\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "#\n",
    "\n",
    "training_corpus=[\"The Limmat flows out of the lake.\", \n",
    "                 \"The bears are in the bear pit near the river.\",\n",
    "                 \"The Rhône flows out of Lake Geneva.\",]\n",
    "training_labels=[\"zurich\", \n",
    "                 \"bern\",\n",
    "                 \"geneva\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to need a helper function that can normalize a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(document, keep_punctuation=False, keep_stop_words=False, keep_inflected=True, keep_numbers=False):\n",
    "    import string\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    # tokenization\n",
    "    word_tokens = word_tokenize(document)\n",
    "\n",
    "    # lemmatization\n",
    "    wl = WordNetLemmatizer()\n",
    "    lemmatize = lambda tokens: [wl.lemmatize(w) for w in tokens]\n",
    "\n",
    "    # stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # normalize the text\n",
    "    normalized = [w.lower() for w in word_tokens\n",
    "                       # lower the text and check if punctuation or stopwords should be kept\n",
    "                       if ((not w.lower() in set(string.punctuation)) or keep_punctuation)\n",
    "                       and\n",
    "                       ((not w.lower() in stop_words) or keep_stop_words)\n",
    "                       and # remove non-alphanumeric tokens\n",
    "                       ((w.lower().isalnum()) or keep_punctuation)\n",
    "                       and # remove numeric tokens\n",
    "                       (not (w.lower().isdigit()) or keep_numbers)\n",
    "                       ] \n",
    "\n",
    "    if not keep_inflected:\n",
    "        normalized = lemmatize(normalized)\n",
    "\n",
    "    return normalized "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does *keep_inflected* affect the output of __normalize__?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        original                      normalized                        inflected\n",
      "0              The Limmat flows out of the lake.            [limmat, flow, lake]            [limmat, flows, lake]\n",
      "1  The bears are in the bear pit near the river.  [bear, bear, pit, near, river]  [bears, bear, pit, near, river]\n",
      "2            The Rhône flows out of Lake Geneva.     [rhône, flow, lake, geneva]     [rhône, flows, lake, geneva]\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_REMOVE\n",
    "normalized_training_corpus = [normalize(item, keep_inflected=False) for item in training_corpus]    \n",
    "inflected_training_corpus = [normalize(item, keep_inflected=True) for item in training_corpus] \n",
    "\n",
    "df=pd.DataFrame(columns=['original', 'normalized', 'inflected'])\n",
    "df['original']=training_corpus\n",
    "df['normalized']=normalized_training_corpus\n",
    "df['inflected']=inflected_training_corpus\n",
    "print (df.to_string())\n",
    "# keep_inflected maintains inflected forms such as 'cities'\n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now, we need to define a __get_vocabulary__ function that gets us all the unique words that appear in the normalized documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary (data):\n",
    "    return list(set(sum(data,[])))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bear', 'limmat', 'river', 'near', 'pit', 'flow', 'lake', 'rhône', 'geneva']\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_REMOVE\n",
    "print(get_vocabulary(normalized_training_corpus))  \n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a class __ms_timer__ that helps us time snippets of code. Its definition follows a special syntax that serves to implement what is known as a context manager. \n",
    "\n",
    "Each code snippet that we wish to time will be placed in an indented block following a __with__ statement. At the end of the indented block, the run time of the snippet will be returned by the class method __get_elapsed_time__. \n",
    "\n",
    "(You can do this same thing effortlessly in an IDE with profiling, but this is a good way to do it in a Jupyter notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class ms_timer:\n",
    "            \n",
    "    def __enter__(self):\n",
    "        self.start=time.time()\n",
    "        return self\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.stop=time.time()\n",
    "    def get_elapsed_time(self):\n",
    "        return 1000*(self.stop-self.start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of how to time code snippets using the context manager trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for the loop: 0.0041 ms\n"
     ]
    }
   ],
   "source": [
    "my_data = range(1, 10)\n",
    "\n",
    "with ms_timer() as timer:\n",
    "    prod=1\n",
    "    for item in my_data:\n",
    "        prod=prod*item\n",
    "print (\"Elapsed time for the loop: \"+str(round(timer.get_elapsed_time(), 4))+\" ms\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNB from scratch\n",
    "\n",
    "We are now ready to implement our MNB from scratch. Our implementation is contained in a class called __naive_bayes__. We can define our class across multiple cells simply by defining a derived class with exactly the same name in the following cells.\n",
    "\n",
    "First we compute the posterior probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class naive_bayes:\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_posterior_probabilities (training_data, verbose=True):\n",
    "\n",
    "        import re\n",
    "\n",
    "        posterior = {}\n",
    "\n",
    "        vocabulary = get_vocabulary(training_data['documents'])\n",
    "        lw = len(vocabulary)\n",
    "\n",
    "        classes = list(set(training_labels))\n",
    "\n",
    "        for index, c in enumerate(classes): \n",
    "\n",
    "            tokens = sum(training_data['documents']\\\n",
    "                                         [training_data['labels']==c], [])\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                den = len(tokens)    \n",
    "            except:\n",
    "                den=0\n",
    "\n",
    "            current_class_docs = tokens\n",
    "\n",
    "            for w in vocabulary:\n",
    "\n",
    "                num = current_class_docs.count(w)                \n",
    "                posterior[(w,c)]=(1+num)/(den+lw)\n",
    "\n",
    "                if verbose:\n",
    "                    \n",
    "                    print ('_'*30)\n",
    "                    message = 'Token '+w+' appears '+str(num)+' times in class '+c\n",
    "                    message=re.sub('1 times', 'once', message)\n",
    "                    print (message)\n",
    "                    \n",
    "                    message = 'There are '+str(den)+' tokens in class '+c\n",
    "                    message=re.sub('are 1 tokens', 'is 1 token', message)\n",
    "                    print (message)\n",
    "                    \n",
    "                    print (current_class_docs)\n",
    "                    print ('Vocab size: '+str(lw))\n",
    "                    print ('Posterior without Laplace smoothing: '+str(num)+'/'+str(den)+'='+str(round(num/den, 2)))\n",
    "                    print ('Posterior with Laplace smoothing: '+str(1+num)+'/'+str(den+lw)+'='+str(round(posterior[(w,c)], 2)))\n",
    "\n",
    "        return posterior  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) What is the posterior probability of finding 'limmat' given that the document is tagged as 'zurich'? Complete the following code snippet to find out. Use *verbose* to see what's going on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________\n",
      "Token bear appears 2 times in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 2/5=0.4\n",
      "Posterior with Laplace smoothing: 3/14=0.21\n",
      "______________________________\n",
      "Token limmat appears 0 times in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/5=0.0\n",
      "Posterior with Laplace smoothing: 1/14=0.07\n",
      "______________________________\n",
      "Token river appears once in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/5=0.2\n",
      "Posterior with Laplace smoothing: 2/14=0.14\n",
      "______________________________\n",
      "Token near appears once in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/5=0.2\n",
      "Posterior with Laplace smoothing: 2/14=0.14\n",
      "______________________________\n",
      "Token pit appears once in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/5=0.2\n",
      "Posterior with Laplace smoothing: 2/14=0.14\n",
      "______________________________\n",
      "Token flow appears 0 times in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/5=0.0\n",
      "Posterior with Laplace smoothing: 1/14=0.07\n",
      "______________________________\n",
      "Token lake appears 0 times in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/5=0.0\n",
      "Posterior with Laplace smoothing: 1/14=0.07\n",
      "______________________________\n",
      "Token rhône appears 0 times in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/5=0.0\n",
      "Posterior with Laplace smoothing: 1/14=0.07\n",
      "______________________________\n",
      "Token geneva appears 0 times in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/5=0.0\n",
      "Posterior with Laplace smoothing: 1/14=0.07\n",
      "______________________________\n",
      "Token bear appears 0 times in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/4=0.0\n",
      "Posterior with Laplace smoothing: 1/13=0.08\n",
      "______________________________\n",
      "Token limmat appears 0 times in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/4=0.0\n",
      "Posterior with Laplace smoothing: 1/13=0.08\n",
      "______________________________\n",
      "Token river appears 0 times in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/4=0.0\n",
      "Posterior with Laplace smoothing: 1/13=0.08\n",
      "______________________________\n",
      "Token near appears 0 times in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/4=0.0\n",
      "Posterior with Laplace smoothing: 1/13=0.08\n",
      "______________________________\n",
      "Token pit appears 0 times in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/4=0.0\n",
      "Posterior with Laplace smoothing: 1/13=0.08\n",
      "______________________________\n",
      "Token flow appears once in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/4=0.25\n",
      "Posterior with Laplace smoothing: 2/13=0.15\n",
      "______________________________\n",
      "Token lake appears once in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/4=0.25\n",
      "Posterior with Laplace smoothing: 2/13=0.15\n",
      "______________________________\n",
      "Token rhône appears once in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/4=0.25\n",
      "Posterior with Laplace smoothing: 2/13=0.15\n",
      "______________________________\n",
      "Token geneva appears once in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/4=0.25\n",
      "Posterior with Laplace smoothing: 2/13=0.15\n",
      "______________________________\n",
      "Token bear appears 0 times in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/3=0.0\n",
      "Posterior with Laplace smoothing: 1/12=0.08\n",
      "______________________________\n",
      "Token limmat appears once in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/3=0.33\n",
      "Posterior with Laplace smoothing: 2/12=0.17\n",
      "______________________________\n",
      "Token river appears 0 times in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/3=0.0\n",
      "Posterior with Laplace smoothing: 1/12=0.08\n",
      "______________________________\n",
      "Token near appears 0 times in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/3=0.0\n",
      "Posterior with Laplace smoothing: 1/12=0.08\n",
      "______________________________\n",
      "Token pit appears 0 times in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/3=0.0\n",
      "Posterior with Laplace smoothing: 1/12=0.08\n",
      "______________________________\n",
      "Token flow appears once in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/3=0.33\n",
      "Posterior with Laplace smoothing: 2/12=0.17\n",
      "______________________________\n",
      "Token lake appears once in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/3=0.33\n",
      "Posterior with Laplace smoothing: 2/12=0.17\n",
      "______________________________\n",
      "Token rhône appears 0 times in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/3=0.0\n",
      "Posterior with Laplace smoothing: 1/12=0.08\n",
      "______________________________\n",
      "Token geneva appears 0 times in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/3=0.0\n",
      "Posterior with Laplace smoothing: 1/12=0.08\n",
      "0.17\n"
     ]
    }
   ],
   "source": [
    "# The method get_posterior_probabilities expects the training data in the form of a data frame\n",
    "training_data = pd.DataFrame(columns=['documents', 'labels'])\n",
    "training_data['documents']=normalized_training_corpus\n",
    "training_data['labels']=training_labels\n",
    "\n",
    "# BEGIN_REMOVE\n",
    "posterior=naive_bayes.get_posterior_probabilities(training_data, verbose=True)\n",
    "print (\"%.2f\"%posterior['limmat', 'zurich'])\n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code so we can train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class naive_bayes(naive_bayes):\n",
    "    \n",
    "    def train(self, training_data, timing=False, verbose=False):\n",
    "\n",
    "            classes = training_data['labels']\n",
    "\n",
    "            # BEGIN_REMOVE            \n",
    "            with ms_timer() as timer:\n",
    "\n",
    "                P_c = \\\n",
    "                [(training_data['labels']==tagged_class).sum()/len(training_data) \\\n",
    "                 for tagged_class in classes]\n",
    "            if timing and verbose:\n",
    "                print('Priors probabilities computed in '+\"%.2f\"%timer.get_elapsed_time()+\" ms\")\n",
    "            \n",
    "            with ms_timer() as timer:\n",
    "                \n",
    "                posterior_p=self.get_posterior_probabilities(training_data, verbose=verbose)\n",
    "            if timing and verbose:    \n",
    "                print('Posterior probabilities computed in '+\"%.2f\"%timer.get_elapsed_time()+\" ms\")    \n",
    "            # END_REMOVE\n",
    "            \n",
    "            return P_c, posterior_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get to train the classifier. \n",
    "\n",
    "Print out the prior probabilities and the posterior probabilities and answer the following questions:\n",
    "\n",
    "a) What is the lowest posterior probability that you observe and why?\n",
    "\n",
    "b) What is the highest posterior probability that you observe and why?\n",
    "\n",
    "c) Why are the prior probabilities all 1/3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors probabilities computed in 0.42 ms\n",
      "______________________________\n",
      "Token bear appears 2 times in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 2/5=0.4\n",
      "Posterior with Laplace smoothing: 3/14=0.21\n",
      "______________________________\n",
      "Token limmat appears 0 times in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/5=0.0\n",
      "Posterior with Laplace smoothing: 1/14=0.07\n",
      "______________________________\n",
      "Token river appears once in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/5=0.2\n",
      "Posterior with Laplace smoothing: 2/14=0.14\n",
      "______________________________\n",
      "Token near appears once in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/5=0.2\n",
      "Posterior with Laplace smoothing: 2/14=0.14\n",
      "______________________________\n",
      "Token pit appears once in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/5=0.2\n",
      "Posterior with Laplace smoothing: 2/14=0.14\n",
      "______________________________\n",
      "Token flow appears 0 times in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/5=0.0\n",
      "Posterior with Laplace smoothing: 1/14=0.07\n",
      "______________________________\n",
      "Token lake appears 0 times in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/5=0.0\n",
      "Posterior with Laplace smoothing: 1/14=0.07\n",
      "______________________________\n",
      "Token rhône appears 0 times in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/5=0.0\n",
      "Posterior with Laplace smoothing: 1/14=0.07\n",
      "______________________________\n",
      "Token geneva appears 0 times in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/5=0.0\n",
      "Posterior with Laplace smoothing: 1/14=0.07\n",
      "______________________________\n",
      "Token bear appears 0 times in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/4=0.0\n",
      "Posterior with Laplace smoothing: 1/13=0.08\n",
      "______________________________\n",
      "Token limmat appears 0 times in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/4=0.0\n",
      "Posterior with Laplace smoothing: 1/13=0.08\n",
      "______________________________\n",
      "Token river appears 0 times in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/4=0.0\n",
      "Posterior with Laplace smoothing: 1/13=0.08\n",
      "______________________________\n",
      "Token near appears 0 times in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/4=0.0\n",
      "Posterior with Laplace smoothing: 1/13=0.08\n",
      "______________________________\n",
      "Token pit appears 0 times in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/4=0.0\n",
      "Posterior with Laplace smoothing: 1/13=0.08\n",
      "______________________________\n",
      "Token flow appears once in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/4=0.25\n",
      "Posterior with Laplace smoothing: 2/13=0.15\n",
      "______________________________\n",
      "Token lake appears once in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/4=0.25\n",
      "Posterior with Laplace smoothing: 2/13=0.15\n",
      "______________________________\n",
      "Token rhône appears once in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/4=0.25\n",
      "Posterior with Laplace smoothing: 2/13=0.15\n",
      "______________________________\n",
      "Token geneva appears once in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/4=0.25\n",
      "Posterior with Laplace smoothing: 2/13=0.15\n",
      "______________________________\n",
      "Token bear appears 0 times in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/3=0.0\n",
      "Posterior with Laplace smoothing: 1/12=0.08\n",
      "______________________________\n",
      "Token limmat appears once in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/3=0.33\n",
      "Posterior with Laplace smoothing: 2/12=0.17\n",
      "______________________________\n",
      "Token river appears 0 times in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/3=0.0\n",
      "Posterior with Laplace smoothing: 1/12=0.08\n",
      "______________________________\n",
      "Token near appears 0 times in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/3=0.0\n",
      "Posterior with Laplace smoothing: 1/12=0.08\n",
      "______________________________\n",
      "Token pit appears 0 times in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/3=0.0\n",
      "Posterior with Laplace smoothing: 1/12=0.08\n",
      "______________________________\n",
      "Token flow appears once in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/3=0.33\n",
      "Posterior with Laplace smoothing: 2/12=0.17\n",
      "______________________________\n",
      "Token lake appears once in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/3=0.33\n",
      "Posterior with Laplace smoothing: 2/12=0.17\n",
      "______________________________\n",
      "Token rhône appears 0 times in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/3=0.0\n",
      "Posterior with Laplace smoothing: 1/12=0.08\n",
      "______________________________\n",
      "Token geneva appears 0 times in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/3=0.0\n",
      "Posterior with Laplace smoothing: 1/12=0.08\n",
      "Posterior probabilities computed in 8.63 ms\n",
      "Prior probabilities:\n",
      "['0.33', '0.33', '0.33']\n",
      "Posterior probabilities:\n",
      "      (token, class)  post_p\n",
      "0       (bear, bern)    0.21\n",
      "1     (limmat, bern)    0.07\n",
      "2      (river, bern)    0.14\n",
      "3       (near, bern)    0.14\n",
      "4        (pit, bern)    0.14\n",
      "5       (flow, bern)    0.07\n",
      "6       (lake, bern)    0.07\n",
      "7      (rhône, bern)    0.07\n",
      "8     (geneva, bern)    0.07\n",
      "9     (bear, geneva)    0.08\n",
      "10  (limmat, geneva)    0.08\n",
      "11   (river, geneva)    0.08\n",
      "12    (near, geneva)    0.08\n",
      "13     (pit, geneva)    0.08\n",
      "14    (flow, geneva)    0.15\n",
      "15    (lake, geneva)    0.15\n",
      "16   (rhône, geneva)    0.15\n",
      "17  (geneva, geneva)    0.15\n",
      "18    (bear, zurich)    0.08\n",
      "19  (limmat, zurich)    0.17\n",
      "20   (river, zurich)    0.08\n",
      "21    (near, zurich)    0.08\n",
      "22     (pit, zurich)    0.08\n",
      "23    (flow, zurich)    0.17\n",
      "24    (lake, zurich)    0.17\n",
      "25   (rhône, zurich)    0.08\n",
      "26  (geneva, zurich)    0.08\n",
      "______________________________\n",
      "Sorted in descending order:\n",
      "      (token, class)  post_p\n",
      "0       (bear, bern)    0.21\n",
      "24    (lake, zurich)    0.17\n",
      "23    (flow, zurich)    0.17\n",
      "19  (limmat, zurich)    0.17\n",
      "16   (rhône, geneva)    0.15\n",
      "17  (geneva, geneva)    0.15\n",
      "14    (flow, geneva)    0.15\n",
      "15    (lake, geneva)    0.15\n",
      "2      (river, bern)    0.14\n",
      "3       (near, bern)    0.14\n",
      "4        (pit, bern)    0.14\n",
      "25   (rhône, zurich)    0.08\n",
      "22     (pit, zurich)    0.08\n",
      "21    (near, zurich)    0.08\n",
      "20   (river, zurich)    0.08\n",
      "18    (bear, zurich)    0.08\n",
      "13     (pit, geneva)    0.08\n",
      "12    (near, geneva)    0.08\n",
      "11   (river, geneva)    0.08\n",
      "10  (limmat, geneva)    0.08\n",
      "9     (bear, geneva)    0.08\n",
      "26  (geneva, zurich)    0.08\n",
      "1     (limmat, bern)    0.07\n",
      "8     (geneva, bern)    0.07\n",
      "7      (rhône, bern)    0.07\n",
      "6       (lake, bern)    0.07\n",
      "5       (flow, bern)    0.07\n"
     ]
    }
   ],
   "source": [
    "nb=naive_bayes()\n",
    "P_c, posterior_p=nb.train(training_data, timing=True, verbose=True) \n",
    "\n",
    "# BEGIN_REMOVE\n",
    "print ('Prior probabilities:')\n",
    "print ([str(round(x, 2)) for x in P_c])\n",
    "\n",
    "print ('Posterior probabilities:')\n",
    "df=pd.DataFrame()\n",
    "df['(token, class)']=[x for x in posterior_p.keys()]\n",
    "df['post_p']=list(map(lambda x:round(x, 2), posterior_p.values()))\n",
    "print (df.to_string())\n",
    "print ('_'*30)\n",
    "print ('Sorted in descending order:')\n",
    "print (df.sort_values(by='post_p', ascending=False).to_string())\n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get to do the classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class naive_bayes(naive_bayes):\n",
    "    \n",
    "    \n",
    "    def classify_document (self, training_data, test_document, verbose=False):\n",
    "\n",
    "            from functools import reduce\n",
    "            import math\n",
    "            from nltk.tokenize import word_tokenize\n",
    "\n",
    "            classes = list(set(training_data['labels']))\n",
    "            \n",
    "            P_c, posterior_p=self.train(training_data, verbose=verbose)\n",
    "\n",
    "            NB=dict()\n",
    "            \n",
    "            normalized_test_document = normalize(test_document, keep_inflected=False)\n",
    "            \n",
    "            if verbose:\n",
    "                print ('_'*30)\n",
    "                print ('Test doc: ', test_document)\n",
    "                print ('Normalized test doc: ', normalized_test_document)\n",
    "\n",
    "            for index, c in enumerate(classes):\n",
    "                \n",
    "                print ('Class: ', c)\n",
    "\n",
    "                posterior_logsum=0\n",
    "                \n",
    "                for token in normalized_test_document:\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print ('Token: ', token)\n",
    "                        \n",
    "                    try:\n",
    "                        posterior_logsum=posterior_logsum+math.log(posterior_p[token, c], 10)\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print ('Posterior ('+token+', '+c+'): '+str(posterior_p[token, c]))\n",
    "                            print ('Posterior logsum: ', posterior_logsum)\n",
    "                            \n",
    "                    except:\n",
    "                        if verbose:\n",
    "                            print ('Token not in training ')\n",
    "                    \n",
    "                if posterior_logsum == 0 and verbose:\n",
    "                        print('Classification failure: insufficient info')\n",
    "            \n",
    "                NB[c]=round(posterior_logsum+math.log(P_c[index], 10), 2)\n",
    "                \n",
    "                if verbose:\n",
    "                    print ('Class: ', c)\n",
    "                    print ('NB: ', NB[c])\n",
    "                \n",
    "            return max(NB, key=NB.get), NB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your classifier with the test document *The name of the city comes from the word 'bear'.* What goes wrong? Can you fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Classification completed in 2.62 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the city comes from the word 'bear'\n",
      "Class:  bern\n",
      "Class:  geneva\n",
      "Class:  zurich\n",
      "bern\n",
      "{'bern': -0.48, 'geneva': -0.48, 'zurich': -0.48}\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_REMOVE\n",
    "\n",
    "import logging\n",
    "nb=naive_bayes()\n",
    "test_corpus = \"The name of the city comes from the word 'bear'\"\n",
    "test_labels = \"bern\"\n",
    "\n",
    "print (test_corpus)\n",
    "\n",
    "with ms_timer() as timer:\n",
    "    result, NB = nb.classify_document(training_data, test_corpus, verbose=False)\n",
    "    \n",
    "logging.warning('Classification completed in '+\"%.2f\"%timer.get_elapsed_time()+\" ms\")    \n",
    "print (result)   \n",
    "print (NB)\n",
    "\n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Classification completed in 18.73 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the city comes from the word bear\n",
      "______________________________\n",
      "Token bear appears 2 times in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 2/5=0.4\n",
      "Posterior with Laplace smoothing: 3/14=0.21\n",
      "______________________________\n",
      "Token limmat appears 0 times in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/5=0.0\n",
      "Posterior with Laplace smoothing: 1/14=0.07\n",
      "______________________________\n",
      "Token river appears once in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/5=0.2\n",
      "Posterior with Laplace smoothing: 2/14=0.14\n",
      "______________________________\n",
      "Token near appears once in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/5=0.2\n",
      "Posterior with Laplace smoothing: 2/14=0.14\n",
      "______________________________\n",
      "Token pit appears once in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/5=0.2\n",
      "Posterior with Laplace smoothing: 2/14=0.14\n",
      "______________________________\n",
      "Token flow appears 0 times in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/5=0.0\n",
      "Posterior with Laplace smoothing: 1/14=0.07\n",
      "______________________________\n",
      "Token lake appears 0 times in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/5=0.0\n",
      "Posterior with Laplace smoothing: 1/14=0.07\n",
      "______________________________\n",
      "Token rhône appears 0 times in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/5=0.0\n",
      "Posterior with Laplace smoothing: 1/14=0.07\n",
      "______________________________\n",
      "Token geneva appears 0 times in class bern\n",
      "There are 5 tokens in class bern\n",
      "['bear', 'bear', 'pit', 'near', 'river']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/5=0.0\n",
      "Posterior with Laplace smoothing: 1/14=0.07\n",
      "______________________________\n",
      "Token bear appears 0 times in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/4=0.0\n",
      "Posterior with Laplace smoothing: 1/13=0.08\n",
      "______________________________\n",
      "Token limmat appears 0 times in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/4=0.0\n",
      "Posterior with Laplace smoothing: 1/13=0.08\n",
      "______________________________\n",
      "Token river appears 0 times in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/4=0.0\n",
      "Posterior with Laplace smoothing: 1/13=0.08\n",
      "______________________________\n",
      "Token near appears 0 times in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/4=0.0\n",
      "Posterior with Laplace smoothing: 1/13=0.08\n",
      "______________________________\n",
      "Token pit appears 0 times in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/4=0.0\n",
      "Posterior with Laplace smoothing: 1/13=0.08\n",
      "______________________________\n",
      "Token flow appears once in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/4=0.25\n",
      "Posterior with Laplace smoothing: 2/13=0.15\n",
      "______________________________\n",
      "Token lake appears once in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/4=0.25\n",
      "Posterior with Laplace smoothing: 2/13=0.15\n",
      "______________________________\n",
      "Token rhône appears once in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/4=0.25\n",
      "Posterior with Laplace smoothing: 2/13=0.15\n",
      "______________________________\n",
      "Token geneva appears once in class geneva\n",
      "There are 4 tokens in class geneva\n",
      "['rhône', 'flow', 'lake', 'geneva']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/4=0.25\n",
      "Posterior with Laplace smoothing: 2/13=0.15\n",
      "______________________________\n",
      "Token bear appears 0 times in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/3=0.0\n",
      "Posterior with Laplace smoothing: 1/12=0.08\n",
      "______________________________\n",
      "Token limmat appears once in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/3=0.33\n",
      "Posterior with Laplace smoothing: 2/12=0.17\n",
      "______________________________\n",
      "Token river appears 0 times in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/3=0.0\n",
      "Posterior with Laplace smoothing: 1/12=0.08\n",
      "______________________________\n",
      "Token near appears 0 times in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/3=0.0\n",
      "Posterior with Laplace smoothing: 1/12=0.08\n",
      "______________________________\n",
      "Token pit appears 0 times in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/3=0.0\n",
      "Posterior with Laplace smoothing: 1/12=0.08\n",
      "______________________________\n",
      "Token flow appears once in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/3=0.33\n",
      "Posterior with Laplace smoothing: 2/12=0.17\n",
      "______________________________\n",
      "Token lake appears once in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 1/3=0.33\n",
      "Posterior with Laplace smoothing: 2/12=0.17\n",
      "______________________________\n",
      "Token rhône appears 0 times in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/3=0.0\n",
      "Posterior with Laplace smoothing: 1/12=0.08\n",
      "______________________________\n",
      "Token geneva appears 0 times in class zurich\n",
      "There are 3 tokens in class zurich\n",
      "['limmat', 'flow', 'lake']\n",
      "Vocab size: 9\n",
      "Posterior without Laplace smoothing: 0/3=0.0\n",
      "Posterior with Laplace smoothing: 1/12=0.08\n",
      "______________________________\n",
      "Test doc:  The name of the city comes from the word bear\n",
      "Normalized test doc:  ['name', 'city', 'come', 'word', 'bear']\n",
      "Class:  bern\n",
      "Token:  name\n",
      "Token not in training \n",
      "Token:  city\n",
      "Token not in training \n",
      "Token:  come\n",
      "Token not in training \n",
      "Token:  word\n",
      "Token not in training \n",
      "Token:  bear\n",
      "Posterior (bear, bern): 0.21428571428571427\n",
      "Posterior logsum:  -0.6690067809585756\n",
      "Class:  bern\n",
      "NB:  -1.15\n",
      "Class:  geneva\n",
      "Token:  name\n",
      "Token not in training \n",
      "Token:  city\n",
      "Token not in training \n",
      "Token:  come\n",
      "Token not in training \n",
      "Token:  word\n",
      "Token not in training \n",
      "Token:  bear\n",
      "Posterior (bear, geneva): 0.07692307692307693\n",
      "Posterior logsum:  -1.1139433523068367\n",
      "Class:  geneva\n",
      "NB:  -1.59\n",
      "Class:  zurich\n",
      "Token:  name\n",
      "Token not in training \n",
      "Token:  city\n",
      "Token not in training \n",
      "Token:  come\n",
      "Token not in training \n",
      "Token:  word\n",
      "Token not in training \n",
      "Token:  bear\n",
      "Posterior (bear, zurich): 0.08333333333333333\n",
      "Posterior logsum:  -1.0791812460476247\n",
      "Class:  zurich\n",
      "NB:  -1.56\n",
      "('bern', {'bern': -1.15, 'geneva': -1.59, 'zurich': -1.56})\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_REMOVE\n",
    "\n",
    "import logging\n",
    "nb=naive_bayes()\n",
    "test_corpus = \"The name of the city comes from the word 'bear'\"\n",
    "test_labels = \"bern\"\n",
    "\n",
    "test_corpus=test_corpus.replace('\\'', '')\n",
    "\n",
    "print (test_corpus)\n",
    "\n",
    "with ms_timer() as timer:\n",
    "    result = nb.classify_document(training_data, test_corpus, verbose=True)\n",
    "    \n",
    "logging.warning('Classification completed in '+\"%.2f\"%timer.get_elapsed_time()+\" ms\")    \n",
    "print (result)    \n",
    "\n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you explain the performance of your classifier on the following test corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Classification of \"We saw the bears there.\" completed in 1.91 ms\n",
      "WARNING:root:Classification of \"We crossed the Rhône.\" completed in 2.07 ms\n",
      "WARNING:root:Classification of \"There is no lake.\" completed in 1.91 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Classifying: We saw the bears there.\n",
      "Class:  bern\n",
      "Class:  geneva\n",
      "Class:  zurich\n",
      "('bern', {'bern': -1.15, 'geneva': -1.59, 'zurich': -1.56})\n",
      "correct label: bern\n",
      "\n",
      " Classifying: We crossed the Rhône.\n",
      "Class:  bern\n",
      "Class:  geneva\n",
      "Class:  zurich\n",
      "('geneva', {'bern': -1.62, 'geneva': -1.29, 'zurich': -1.56})\n",
      "correct label: geneva\n",
      "\n",
      " Classifying: There is no lake.\n",
      "Class:  bern\n",
      "Class:  geneva\n",
      "Class:  zurich\n",
      "('zurich', {'bern': -1.62, 'geneva': -1.29, 'zurich': -1.26})\n",
      "correct label: bern\n"
     ]
    }
   ],
   "source": [
    "test_corpus = ['We saw the bears there.', \n",
    "               'We crossed the Rhône.', \n",
    "               'There is no lake.',\n",
    "              ]\n",
    "test_labels = ['bern',\n",
    "               'geneva',\n",
    "               'bern',\n",
    "              ]\n",
    "\n",
    "nb=naive_bayes() \n",
    "\n",
    "\n",
    "\n",
    "for item in test_corpus:\n",
    "    print ('\\n Classifying: '+item)\n",
    "    with ms_timer() as timer:\n",
    "        result = nb.classify_document(training_data, item)\n",
    "    logging.warning('Classification of \\\"'+item+'\\\" completed in '+\"%.2f\"%timer.get_elapsed_time()+\" ms\")    \n",
    "    print (result)                                      \n",
    "    print ('correct label: '+test_labels[test_corpus.index(item)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your classifier with the one-sentence document \"The federal capital is pretty.\" What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  bern\n",
      "Class:  geneva\n",
      "Class:  zurich\n",
      "('bern', {'bern': -0.48, 'geneva': -0.48, 'zurich': -0.48})\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_REMOVE\n",
    "test_corpus = \"The federal capital is pretty.\"\n",
    "test_labels = \"bern\"\n",
    "\n",
    "# Your classifier fails because your test document contains a previously unseen word. \n",
    "print (nb.classify_document(training_data, test_corpus))\n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        rigi       1.00      0.50      0.67         2\n",
      "     pilatus       0.50      1.00      0.67         1\n",
      "        dole       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.83      0.83      0.78         4\n",
      "weighted avg       0.88      0.75      0.75         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_corpus = ['It’s surrounded by three different lakes.', \n",
    "               'You can hike to the top from Goldau.', \n",
    "               'It’s across the lake from Rigi.',\n",
    "               'Meteoschweiz has a radar at the top.',\n",
    "              ]\n",
    "test_labels = ['rigi',\n",
    "               'rigi',\n",
    "               'pilatus',\n",
    "               'dole',\n",
    "              ]\n",
    "\n",
    "predicted_labels = ['pilatus',\n",
    "               'rigi',\n",
    "               'pilatus',\n",
    "               'dole',\n",
    "              ]\n",
    "\n",
    "target_names = sorted(list(set(test_labels)))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['rigi', 'pilatus', 'dole']\n",
    "print(classification_report(test_labels, predicted_labels, labels = target_names, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
