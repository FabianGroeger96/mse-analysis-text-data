{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://www.msengineering.ch/typo3conf/ext/nm_theme_msengineering/Resources/Public/Template/img/mse_logo.jpg \"MSE Logo\") \n",
    "\n",
    "# AnTeDe Practical Work 1c : A New Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start over with a new example: Web page \n",
    "**Redo the same operations** as in the previous notebook (Lab 1b) using this time a Wikipedia page (i.e. in HTML, not raw text format).  The content and even language are your choice, but we suggest using the English Wikipedia page on Switzerland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from urllib import request\n",
    "import matplotlib.pyplot \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With inspiration from [Chapter 3 of the NLTK book](http://www.nltk.org/book/ch03.html), write the instruction for performing a request to obtain the file found at `url2`.  What is the length of the resulting string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request \n",
    "# refer to the NLTK book Ch. 3, for dealing with HTML\n",
    "url2 = \"https://en.wikipedia.org/wiki/Switzerland\" \n",
    "response2 = request.urlopen(url2)\n",
    "html2 = response2.read().decode('utf8')\n",
    "print(len(html2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `BeautifulSoup` Python package, please extract the text from the HTML page.  (Assuming the entire page is stored in `html2`, the text will be extracted in `raw2`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2 = BeautifulSoup(html2).get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please reuse the technique from the previous notebook (Lab 1b) to trim the beginning and end of the string `raw2`, removing the text which is not logically part of the Wikipedia article.  How long is the resulting text?  Please display a fragment from the start and one from the end (calling it e.g. `raw2trimmed`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segment the text into sentence, then tokenize each sentence.  Write the result (one sentence per line, whitespaces between tokens) in a file called `sample_web_page.txt` and open it using a text editor.  Do you see any mistakes?  Can you clean some of them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sample_web_page.txt\"\n",
    "# For a local file, this is the relative path with respect to the notebook\n",
    "# For Google Colab, use e.g.: /content/gdrive/My Drive/sample_web_page.txt\n",
    "if os.path.exists(filename): \n",
    "    os.remove(filename)\n",
    "fd = open(filename, 'a', encoding='utf8')\n",
    "# Please write your Python code below and execute it.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now tokenize the initial string without performing sentence segmentation, and store the result as e.g. `words2` without writing it in a file.  How many tokens are there in this text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please create again an `nltk.Text`object from the list of tokens called `words2`.  Apply to it the functions: `concordance`, `similar` and `collocations`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please compute the vocabulary of your text, as in 1b.  How many (different) words does it contain?  (This includes punctuations and other marks identified in tokenization.  Capitals are different from low-case.)  Which words of more than 15 letters do appear in the vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a `FreqDist` object with the words from this page, by lowercasing all words that contain only letters (to find them, use the `.isalpha()` method in Python).  What are the 30 most frequent words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please display the cumulative plot of the number of occurrences of words, limited to the 70 most frequent ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you build a list with the length of each word (instead of the word), then use this list to create a new `FreqDist` object, and plot the frequency distribution?  (Not the cumulative one.)  What is the most frequent length?  What can you observe about the ordering of the lengths by decreasing frequency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zipf's Law.** As in Lab 1b, please produce a list of the number of occurrences of each word, in decreasing order, then plot (for about 100 ranks) the number of occurrences on the *y&nbsp;* axis and the rank of each value (1st, 2nd, 3rd, ...) on the *x&nbsp;* axis.  Add a curve of the shape *y = a/(x+b)*, trying to fit *a&nbsp;* and *b&nbsp;* as close as you can so that the two curves look superposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Lab 1c\n",
    "Please save the completed notebook, add it to a *zip* file with part 1b, and upload them to Moodle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
